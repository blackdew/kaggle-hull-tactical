{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "env-setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Ensure repo root is importable (for kaggle_evaluation)\n",
    "import sys, os\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 공개 솔루션들:\n",
    "\n",
    "- [모델 7](#Model_7) - LB=[17.396](https://www.kaggle.com/code/baidalinadilzhan/hull-tactical-lb-17-396?scriptVersionId=262804590) - v.1 - [hull-tactical-lb-17.396](https://www.kaggle.com/code/baidalinadilzhan/hull-tactical-lb-17-396)\n",
    "- [모델 6](#Model_6) - LB=[10.237](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol?scriptVersionId=262531157) - v.3 - [Hull Tactical - Leaderboard LOL](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol).2\n",
    "- [모델 5](#Model_5) - LB=[10.217](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard?scriptVersionId=262493413) - v.9 - [Hull Tactical - Max Leaderboard](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard)\n",
    "- [모델 4](#Model_4) - LB=[10.164](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard?scriptVersionId=262493413) - v.4 - [Hull Tactical - Max Leaderboard](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard)\n",
    "- [모델 1](#Model_1) - LB=[10.147](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol?scriptVersionId=262460746) - v.1 - [Hull Tactical - Leaderboard LOL](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol).1\n",
    "- [모델 2](#Model_2) - LB=[10.005](https://www.kaggle.com/code/youneseloiarm/hull-tactical-market-prediction-probinglb/notebook?scriptVersionId=262450829) - v.4 - [Hull Tactical - Market Prediction - ProbingLB](https://www.kaggle.com/code/youneseloiarm/hull-tactical-market-prediction-probinglb)\n",
    "- [모델 3](#Model_3) - LB=[ &nbsp;8.093](https://www.kaggle.com/code/imaadmahmood/hull-market-prediction?scriptVersionId=262297550) - v.4 - [Hull Market Prediction](https://www.kaggle.com/code/imaadmahmood/hull-market-predictionb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "source": [
    "이 컨테스트에서는 모든 테스트 데이터가 훈련 세트에 포함되어 있어 리더보드가 실제로는 중요하지 않습니다. 단순히 '미래' 시장 행동에 대한 완벽한 지식이 있다면 메트릭의 최대 가능 점수가 얼마인지 궁금했고, 평가 메트릭이 어떻게 작동하는지 더 잘 이해하고 싶었습니다.\n",
    "\n",
    "(그리고 비록 짧은 시간이었지만, 내 인생에서 적어도 한 번은 리더보드 1위에 오르는 것도 재미있었습니다 =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "MAX_INVESTMENT = 2\n",
    "MIN_INVESTMENT = 0\n",
    "DATA_PATH: Path = Path('../data')\n",
    "\n",
    "_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n",
    "\n",
    "true_targets = {\n",
    "    int(d): float(v)\n",
    "    for d, v in zip(\n",
    "        _true_train_df[\"date_id\"].to_numpy(),\n",
    "        _true_train_df[\"forward_returns\"].to_numpy()\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict_Model_1(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    t = true_targets.get(date_id, None)  \n",
    "    pred = MAX_INVESTMENT if t > 0 else MIN_INVESTMENT\n",
    "    print(f'{pred}')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8_990, 98)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>D1</th><th>D2</th><th>D3</th><th>D4</th><th>D5</th><th>D6</th><th>D7</th><th>D8</th><th>D9</th><th>E1</th><th>E10</th><th>E11</th><th>E12</th><th>E13</th><th>E14</th><th>E15</th><th>E16</th><th>E17</th><th>E18</th><th>E19</th><th>E2</th><th>E20</th><th>E3</th><th>E4</th><th>E5</th><th>E6</th><th>E7</th><th>E8</th><th>E9</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>&hellip;</th><th>P13</th><th>P2</th><th>P3</th><th>P4</th><th>P5</th><th>P6</th><th>P7</th><th>P8</th><th>P9</th><th>S1</th><th>S10</th><th>S11</th><th>S12</th><th>S2</th><th>S3</th><th>S4</th><th>S5</th><th>S6</th><th>S7</th><th>S8</th><th>S9</th><th>V1</th><th>V10</th><th>V11</th><th>V12</th><th>V13</th><th>V2</th><th>V3</th><th>V4</th><th>V5</th><th>V6</th><th>V7</th><th>V8</th><th>V9</th><th>forward_returns</th><th>risk_free_rate</th><th>market_forward_excess_returns</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.002421</td><td>0.000301</td><td>-0.003038</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.008495</td><td>0.000303</td><td>-0.009114</td></tr><tr><td>2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.009624</td><td>0.000301</td><td>-0.010243</td></tr><tr><td>3</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.004662</td><td>0.000299</td><td>0.004046</td></tr><tr><td>4</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.011686</td><td>0.000299</td><td>-0.012301</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>8985</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.56537850833686&quot;</td><td>&quot;0.18452380952381&quot;</td><td>&quot;0.0191798941798942&quot;</td><td>&quot;0.0191798941798942&quot;</td><td>&quot;0.00595238095238095&quot;</td><td>&quot;0.00595238095238095&quot;</td><td>&quot;0.911375661375661&quot;</td><td>&quot;-0.0834957603011584&quot;</td><td>&quot;-0.572446603148107&quot;</td><td>&quot;0.22363818831577&quot;</td><td>&quot;-0.122313603525027&quot;</td><td>&quot;1.20925014422219&quot;</td><td>&quot;1.54011631243565&quot;</td><td>&quot;1.6551743632761&quot;</td><td>&quot;0.0314153439153439&quot;</td><td>&quot;0.331679894179894&quot;</td><td>&quot;0.0347222222222222&quot;</td><td>&quot;0.0382692660297448&quot;</td><td>&quot;-0.301875981317616&quot;</td><td>&quot;0.91468253968254&quot;</td><td>&quot;0.274140211640212&quot;</td><td>&quot;0.984115038349353&quot;</td><td>&quot;0.0806878306878307&quot;</td><td>&quot;0.476521164021164&quot;</td><td>&quot;0.597442456305455&quot;</td><td>&quot;0.718253968253968&quot;</td><td>&quot;0.238756613756614&quot;</td><td>&hellip;</td><td>&quot;0.625330687830688&quot;</td><td>&quot;-1.35449847084931&quot;</td><td>&quot;0.0462962962962963&quot;</td><td>&quot;0.514550264550265&quot;</td><td>&quot;0.276768769975892&quot;</td><td>&quot;-0.261325600057626&quot;</td><td>&quot;0.811753887240293&quot;</td><td>&quot;1.78492936574625&quot;</td><td>&quot;0.0396825396825397&quot;</td><td>&quot;0.249933173088953&quot;</td><td>&quot;0.273148148148148&quot;</td><td>&quot;0.134920634920635&quot;</td><td>&quot;0.634464741288425&quot;</td><td>&quot;-0.446681908068479&quot;</td><td>&quot;-0.0526855763278288&quot;</td><td>&quot;0.083994708994709&quot;</td><td>&quot;0.055281954303961&quot;</td><td>&quot;0.209656084656085&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.574660952369144&quot;</td><td>&quot;0.748677248677249&quot;</td><td>&quot;0.498677248677249&quot;</td><td>&quot;-0.616394800009832&quot;</td><td>&quot;0.561838624338624&quot;</td><td>&quot;0.533730158730159&quot;</td><td>&quot;-0.432282453978163&quot;</td><td>&quot;0.78505291005291&quot;</td><td>&quot;0.46957671957672&quot;</td><td>&quot;0.837962962962963&quot;</td><td>&quot;1.22677167174681&quot;</td><td>&quot;0.822751322751323&quot;</td><td>&quot;-0.707360636419722&quot;</td><td>&quot;0.142857142857143&quot;</td><td>&quot;-0.649616421794573&quot;</td><td>0.002457</td><td>0.000155</td><td>0.00199</td></tr><tr><td>8986</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.56294570736285&quot;</td><td>&quot;0.184193121693122&quot;</td><td>&quot;0.0188492063492063&quot;</td><td>&quot;0.0188492063492063&quot;</td><td>&quot;0.00562169312169312&quot;</td><td>&quot;0.00562169312169312&quot;</td><td>&quot;0.911706349206349&quot;</td><td>&quot;-0.0835423385235207&quot;</td><td>&quot;-0.572080270433783&quot;</td><td>&quot;0.222909704235328&quot;</td><td>&quot;-0.732396949860551&quot;</td><td>&quot;1.22545909429746&quot;</td><td>&quot;1.53776136440687&quot;</td><td>&quot;1.67226210824883&quot;</td><td>&quot;0.0310846560846561&quot;</td><td>&quot;0.331349206349206&quot;</td><td>&quot;0.0343915343915344&quot;</td><td>&quot;0.0382047677964104&quot;</td><td>&quot;-0.301897014182979&quot;</td><td>&quot;0.915013227513228&quot;</td><td>&quot;0.26984126984127&quot;</td><td>&quot;0.904452962662242&quot;</td><td>&quot;0.0734126984126984&quot;</td><td>&quot;0.479166666666667&quot;</td><td>&quot;0.605078681561105&quot;</td><td>&quot;0.718253968253968&quot;</td><td>&quot;0.220899470899471&quot;</td><td>&hellip;</td><td>&quot;0.739417989417989&quot;</td><td>&quot;-1.38478508824334&quot;</td><td>&quot;0.232142857142857&quot;</td><td>&quot;0.379298941798942&quot;</td><td>&quot;1.19925967661965&quot;</td><td>&quot;-0.344273917030356&quot;</td><td>&quot;0.690323485609809&quot;</td><td>&quot;1.79159569953088&quot;</td><td>&quot;0.037037037037037&quot;</td><td>&quot;0.298532581791712&quot;</td><td>&quot;0.933201058201058&quot;</td><td>&quot;0.721560846560847&quot;</td><td>&quot;1.21134501564263&quot;</td><td>&quot;-0.118050110853058&quot;</td><td>&quot;-0.249315284179041&quot;</td><td>&quot;0.566798941798942&quot;</td><td>&quot;0.107330113648913&quot;</td><td>&quot;0.228174603174603&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.580932017246943&quot;</td><td>&quot;0.37037037037037&quot;</td><td>&quot;0.528439153439153&quot;</td><td>&quot;-0.642039876784022&quot;</td><td>&quot;0.587632275132275&quot;</td><td>&quot;0.526455026455027&quot;</td><td>&quot;-0.429505792996239&quot;</td><td>&quot;0.767857142857143&quot;</td><td>&quot;0.671957671957672&quot;</td><td>&quot;0.837962962962963&quot;</td><td>&quot;0.785876680219954&quot;</td><td>&quot;0.805555555555556&quot;</td><td>&quot;-0.715692186942146&quot;</td><td>&quot;0.196097883597884&quot;</td><td>&quot;-0.668289260803376&quot;</td><td>0.002312</td><td>0.000156</td><td>0.001845</td></tr><tr><td>8987</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.5605200537219&quot;</td><td>&quot;0.183862433862434&quot;</td><td>&quot;0.0185185185185185&quot;</td><td>&quot;0.0185185185185185&quot;</td><td>&quot;0.00529100529100529&quot;</td><td>&quot;0.00529100529100529&quot;</td><td>&quot;0.912037037037037&quot;</td><td>&quot;-0.0838740749780047&quot;</td><td>&quot;-0.572016205010642&quot;</td><td>&quot;0.222211305740983&quot;</td><td>&quot;-0.800464708548532&quot;</td><td>&quot;1.24727312781993&quot;</td><td>&quot;1.53474183594455&quot;</td><td>&quot;1.69546884570455&quot;</td><td>&quot;0.0307539682539683&quot;</td><td>&quot;0.331018518518519&quot;</td><td>&quot;0.0340608465608466&quot;</td><td>&quot;0.038118211270212&quot;</td><td>&quot;-0.301918047426536&quot;</td><td>&quot;0.915343915343915&quot;</td><td>&quot;0.273148148148148&quot;</td><td>&quot;0.842294693008398&quot;</td><td>&quot;0.0740740740740741&quot;</td><td>&quot;0.478835978835979&quot;</td><td>&quot;0.611319101072675&quot;</td><td>&quot;0.724867724867725&quot;</td><td>&quot;0.223544973544974&quot;</td><td>&hellip;</td><td>&quot;0.809193121693122&quot;</td><td>&quot;-1.42000677294771&quot;</td><td>&quot;0.849867724867725&quot;</td><td>&quot;0.375661375661376&quot;</td><td>&quot;0.429471475390351&quot;</td><td>&quot;-0.233373569970158&quot;</td><td>&quot;-0.289766464491947&quot;</td><td>&quot;1.79281602958001&quot;</td><td>&quot;0.041005291005291&quot;</td><td>&quot;0.371361679434561&quot;</td><td>&quot;0.793650793650794&quot;</td><td>&quot;0.689814814814815&quot;</td><td>&quot;0.885178015455313&quot;</td><td>&quot;-0.316882407040784&quot;</td><td>&quot;-0.422374286909508&quot;</td><td>&quot;0.631613756613757&quot;</td><td>&quot;-0.0297695460919471&quot;</td><td>&quot;0.221891534391534&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.583555500826733&quot;</td><td>&quot;0.477513227513228&quot;</td><td>&quot;0.599206349206349&quot;</td><td>&quot;-0.638658197334489&quot;</td><td>&quot;0.39484126984127&quot;</td><td>&quot;0.433531746031746&quot;</td><td>&quot;-0.425462111645349&quot;</td><td>&quot;0.734126984126984&quot;</td><td>&quot;0.481481481481481&quot;</td><td>&quot;0.787698412698413&quot;</td><td>&quot;0.834897865394715&quot;</td><td>&quot;0.823412698412698&quot;</td><td>&quot;-0.723948535462705&quot;</td><td>&quot;0.133928571428571&quot;</td><td>&quot;-0.67094613354537&quot;</td><td>0.002891</td><td>0.000156</td><td>0.002424</td></tr><tr><td>8988</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.55810150804271&quot;</td><td>&quot;0.183531746031746&quot;</td><td>&quot;0.0181878306878307&quot;</td><td>&quot;0.0181878306878307&quot;</td><td>&quot;0.00496031746031746&quot;</td><td>&quot;0.00496031746031746&quot;</td><td>&quot;0.912367724867725&quot;</td><td>&quot;-0.0842058914414559&quot;</td><td>&quot;-0.571952144722391&quot;</td><td>&quot;0.22151267935636&quot;</td><td>&quot;-0.596939053960628&quot;</td><td>&quot;1.27192582796906&quot;</td><td>&quot;1.53234020642116&quot;</td><td>&quot;1.7216916162139&quot;</td><td>&quot;0.0304232804232804&quot;</td><td>&quot;0.330687830687831&quot;</td><td>&quot;0.0337301587301587&quot;</td><td>&quot;0.0376470782051439&quot;</td><td>&quot;-0.301939081048321&quot;</td><td>&quot;0.915674603174603&quot;</td><td>&quot;0.27281746031746&quot;</td><td>&quot;0.858581646251435&quot;</td><td>&quot;0.082010582010582&quot;</td><td>&quot;0.478505291005291&quot;</td><td>&quot;0.604658106925698&quot;</td><td>&quot;0.71957671957672&quot;</td><td>&quot;0.262566137566138&quot;</td><td>&hellip;</td><td>&quot;0.923611111111111&quot;</td><td>&quot;-1.43102829482739&quot;</td><td>&quot;0.303240740740741&quot;</td><td>&quot;0.0687830687830688&quot;</td><td>&quot;0.0448883973623847&quot;</td><td>&quot;-0.26986245435381&quot;</td><td>&quot;0.423267529815228&quot;</td><td>&quot;1.79293416392505&quot;</td><td>&quot;0.046957671957672&quot;</td><td>&quot;0.411609523901239&quot;</td><td>&quot;0.0119047619047619&quot;</td><td>&quot;0.0264550264550265&quot;</td><td>&quot;-0.00178527792358688&quot;</td><td>&quot;-0.31796113525985&quot;</td><td>&quot;-0.608347743326619&quot;</td><td>&quot;0.0661375661375661&quot;</td><td>&quot;-0.00159373754975138&quot;</td><td>&quot;0.259920634920635&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.630089662276381&quot;</td><td>&quot;0.915343915343915&quot;</td><td>&quot;0.462301587301587&quot;</td><td>&quot;-0.626926847514841&quot;</td><td>&quot;0.326388888888889&quot;</td><td>&quot;0.394179894179894&quot;</td><td>&quot;-0.385169943772323&quot;</td><td>&quot;0.69510582010582&quot;</td><td>&quot;0.65542328042328&quot;</td><td>&quot;0.783730158730159&quot;</td><td>&quot;0.9940257708608&quot;</td><td>&quot;0.851851851851852&quot;</td><td>&quot;-0.684937261881957&quot;</td><td>&quot;0.101851851851852&quot;</td><td>&quot;-0.646264996301655&quot;</td><td>0.00831</td><td>0.000156</td><td>0.007843</td></tr><tr><td>8989</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.55569003125934&quot;</td><td>&quot;0.183201058201058&quot;</td><td>&quot;0.0178571428571429&quot;</td><td>&quot;0.0178571428571429&quot;</td><td>&quot;0.00462962962962963&quot;</td><td>&quot;0.00462962962962963&quot;</td><td>&quot;0.912698412698413&quot;</td><td>&quot;-0.0845377880574921&quot;</td><td>&quot;-0.571888089567891&quot;</td><td>&quot;0.22081382366098&quot;</td><td>&quot;-0.74706627431364&quot;</td><td>&quot;1.31262576944327&quot;</td><td>&quot;1.52987532972841&quot;</td><td>&quot;1.76531662714271&quot;</td><td>&quot;0.0300925925925926&quot;</td><td>&quot;0.330357142857143&quot;</td><td>&quot;0.0333994708994709&quot;</td><td>&quot;0.037560950304484&quot;</td><td>&quot;-0.267676958923522&quot;</td><td>&quot;0.916005291005291&quot;</td><td>&quot;0.305886243386243&quot;</td><td>&quot;0.822594079482911&quot;</td><td>&quot;0.0806878306878307&quot;</td><td>&quot;0.478174603174603&quot;</td><td>&quot;0.603072293060331&quot;</td><td>&quot;0.705687830687831&quot;</td><td>&quot;0.263888888888889&quot;</td><td>&hellip;</td><td>&quot;0.886243386243386&quot;</td><td>&quot;-1.46998499265758&quot;</td><td>&quot;0.553571428571429&quot;</td><td>&quot;0.244378306878307&quot;</td><td>&quot;0.306917368407162&quot;</td><td>&quot;-0.177545897600496&quot;</td><td>&quot;1.88554125770069&quot;</td><td>&quot;1.79649150290836&quot;</td><td>&quot;0.0476190476190476&quot;</td><td>&quot;0.410794083459945&quot;</td><td>&quot;0.351851851851852&quot;</td><td>&quot;0.0707671957671958&quot;</td><td>&quot;0.257442926006181&quot;</td><td>&quot;0.0111091009226917&quot;</td><td>&quot;-0.642479736305208&quot;</td><td>&quot;0.170634920634921&quot;</td><td>&quot;-0.105022008724652&quot;</td><td>&quot;0.354166666666667&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.629295245255621&quot;</td><td>&quot;0.771825396825397&quot;</td><td>&quot;0.318783068783069&quot;</td><td>&quot;-0.668049927742716&quot;</td><td>&quot;0.12962962962963&quot;</td><td>&quot;0.370039682539683&quot;</td><td>&quot;-0.451307999749732&quot;</td><td>&quot;0.663359788359788&quot;</td><td>&quot;0.0667989417989418&quot;</td><td>&quot;0.783730158730159&quot;</td><td>&quot;1.06803685027394&quot;</td><td>&quot;0.87962962962963&quot;</td><td>&quot;-0.764805966930975&quot;</td><td>&quot;0.0790343915343915&quot;</td><td>&quot;-0.705661850563573&quot;</td><td>0.000099</td><td>0.000156</td><td>-0.000368</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8_990, 98)\n",
       "┌─────────┬─────┬─────┬─────┬───┬────────────────┬────────────────┬────────────────┬───────────────┐\n",
       "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ V9             ┆ forward_return ┆ risk_free_rate ┆ market_forwar │\n",
       "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---            ┆ s              ┆ ---            ┆ d_excess_retu │\n",
       "│ i64     ┆ i64 ┆ i64 ┆ i64 ┆   ┆ str            ┆ ---            ┆ f64            ┆ rns           │\n",
       "│         ┆     ┆     ┆     ┆   ┆                ┆ f64            ┆                ┆ ---           │\n",
       "│         ┆     ┆     ┆     ┆   ┆                ┆                ┆                ┆ f64           │\n",
       "╞═════════╪═════╪═════╪═════╪═══╪════════════════╪════════════════╪════════════════╪═══════════════╡\n",
       "│ 0       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.002421      ┆ 0.000301       ┆ -0.003038     │\n",
       "│ 1       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.008495      ┆ 0.000303       ┆ -0.009114     │\n",
       "│ 2       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.009624      ┆ 0.000301       ┆ -0.010243     │\n",
       "│ 3       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ 0.004662       ┆ 0.000299       ┆ 0.004046      │\n",
       "│ 4       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.011686      ┆ 0.000299       ┆ -0.012301     │\n",
       "│ …       ┆ …   ┆ …   ┆ …   ┆ … ┆ …              ┆ …              ┆ …              ┆ …             │\n",
       "│ 8985    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.64961642179 ┆ 0.002457       ┆ 0.000155       ┆ 0.00199       │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 4573           ┆                ┆                ┆               │\n",
       "│ 8986    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.66828926080 ┆ 0.002312       ┆ 0.000156       ┆ 0.001845      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 3376           ┆                ┆                ┆               │\n",
       "│ 8987    ┆ 0   ┆ 0   ┆ 1   ┆ … ┆ -0.67094613354 ┆ 0.002891       ┆ 0.000156       ┆ 0.002424      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 537            ┆                ┆                ┆               │\n",
       "│ 8988    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.64626499630 ┆ 0.00831        ┆ 0.000156       ┆ 0.007843      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 1655           ┆                ┆                ┆               │\n",
       "│ 8989    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.70566185056 ┆ 0.000099       ┆ 0.000156       ┆ -0.000368     │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 3573           ┆                ┆                ┆               │\n",
       "└─────────┴─────┴─────┴─────┴───┴────────────────┴────────────────┴────────────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 99)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>D1</th><th>D2</th><th>D3</th><th>D4</th><th>D5</th><th>D6</th><th>D7</th><th>D8</th><th>D9</th><th>E1</th><th>E10</th><th>E11</th><th>E12</th><th>E13</th><th>E14</th><th>E15</th><th>E16</th><th>E17</th><th>E18</th><th>E19</th><th>E2</th><th>E20</th><th>E3</th><th>E4</th><th>E5</th><th>E6</th><th>E7</th><th>E8</th><th>E9</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>&hellip;</th><th>P2</th><th>P3</th><th>P4</th><th>P5</th><th>P6</th><th>P7</th><th>P8</th><th>P9</th><th>S1</th><th>S10</th><th>S11</th><th>S12</th><th>S2</th><th>S3</th><th>S4</th><th>S5</th><th>S6</th><th>S7</th><th>S8</th><th>S9</th><th>V1</th><th>V10</th><th>V11</th><th>V12</th><th>V13</th><th>V2</th><th>V3</th><th>V4</th><th>V5</th><th>V6</th><th>V7</th><th>V8</th><th>V9</th><th>is_scored</th><th>lagged_forward_returns</th><th>lagged_risk_free_rate</th><th>lagged_market_forward_excess_returns</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>8980</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1.577651</td><td>0.186177</td><td>0.001323</td><td>0.001323</td><td>0.001323</td><td>0.001323</td><td>0.955026</td><td>-0.583419</td><td>-0.704264</td><td>0.298365</td><td>-0.691361</td><td>1.259065</td><td>1.556516</td><td>1.71258</td><td>0.033069</td><td>0.333333</td><td>0.036376</td><td>-0.046483</td><td>-0.312326</td><td>0.913029</td><td>0.306217</td><td>1.025756</td><td>0.081349</td><td>0.478175</td><td>0.675627</td><td>0.699735</td><td>0.256283</td><td>&hellip;</td><td>-1.427834</td><td>0.352513</td><td>0.926257</td><td>0.431383</td><td>-0.476976</td><td>0.500245</td><td>1.784173</td><td>0.029762</td><td>0.294719</td><td>0.51455</td><td>0.446429</td><td>0.466551</td><td>0.085717</td><td>-0.230132</td><td>0.272487</td><td>-0.106894</td><td>0.199735</td><td>0.409392</td><td>0.532717</td><td>0.744048</td><td>0.440476</td><td>-0.654839</td><td>0.699735</td><td>0.699074</td><td>-0.5024</td><td>0.882937</td><td>0.892196</td><td>0.828042</td><td>0.999172</td><td>0.759921</td><td>-0.803127</td><td>0.170966</td><td>-0.751909</td><td>true</td><td>0.003541</td><td>0.000161</td><td>0.003068</td></tr><tr><td>8981</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1.575182</td><td>0.185847</td><td>0.000992</td><td>0.000992</td><td>0.000992</td><td>0.000992</td><td>0.955357</td><td>-0.583074</td><td>-0.703759</td><td>0.297608</td><td>-0.504499</td><td>1.193468</td><td>1.554184</td><td>1.640054</td><td>0.032738</td><td>0.333003</td><td>0.036045</td><td>0.073582</td><td>-0.312345</td><td>0.91336</td><td>0.305886</td><td>0.989571</td><td>0.082672</td><td>0.477844</td><td>0.661527</td><td>0.719577</td><td>0.255952</td><td>&hellip;</td><td>-1.37652</td><td>0.953042</td><td>0.386905</td><td>0.523549</td><td>-0.421365</td><td>-0.234829</td><td>1.770175</td><td>0.03373</td><td>0.304496</td><td>0.638228</td><td>0.636905</td><td>1.849101</td><td>0.28169</td><td>-0.041995</td><td>0.448413</td><td>0.094321</td><td>0.215608</td><td>0.409392</td><td>0.597864</td><td>0.872354</td><td>0.691138</td><td>-0.583443</td><td>0.62996</td><td>0.598545</td><td>-0.394268</td><td>0.863757</td><td>0.699074</td><td>0.831349</td><td>1.120336</td><td>0.556217</td><td>-0.686192</td><td>0.141865</td><td>-0.660326</td><td>true</td><td>-0.005964</td><td>0.000162</td><td>-0.006437</td></tr><tr><td>8982</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.57272</td><td>0.185516</td><td>0.000661</td><td>0.000661</td><td>0.000661</td><td>0.000661</td><td>0.955688</td><td>-0.083356</td><td>-0.573546</td><td>0.225822</td><td>-0.393903</td><td>1.123361</td><td>1.551723</td><td>1.562722</td><td>0.032407</td><td>0.332672</td><td>0.035714</td><td>0.033581</td><td>-0.312364</td><td>0.91369</td><td>0.291997</td><td>1.040514</td><td>0.081349</td><td>0.477513</td><td>0.655741</td><td>0.724206</td><td>0.22619</td><td>&hellip;</td><td>-1.34762</td><td>0.210979</td><td>0.635251</td><td>-1.138198</td><td>-0.494248</td><td>-1.042718</td><td>1.754171</td><td>0.032407</td><td>0.257609</td><td>0.082011</td><td>0.152116</td><td>-0.201611</td><td>0.346373</td><td>0.054032</td><td>0.137566</td><td>0.294305</td><td>0.194444</td><td>0.409392</td><td>0.596528</td><td>0.778439</td><td>0.634921</td><td>-0.483236</td><td>0.669974</td><td>0.603836</td><td>-0.17042</td><td>0.848545</td><td>0.647487</td><td>0.832672</td><td>1.088992</td><td>0.665344</td><td>-0.459367</td><td>0.199405</td><td>-0.510979</td><td>true</td><td>-0.00741</td><td>0.00016</td><td>-0.007882</td></tr><tr><td>8983</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.570266</td><td>0.185185</td><td>0.019841</td><td>0.019841</td><td>0.006614</td><td>0.006614</td><td>0.956019</td><td>-0.083403</td><td>-0.57318</td><td>0.225094</td><td>-0.541646</td><td>1.167982</td><td>1.549272</td><td>1.611215</td><td>0.032077</td><td>0.332341</td><td>0.035384</td><td>0.033998</td><td>-0.312383</td><td>0.914021</td><td>0.267196</td><td>1.091376</td><td>0.085979</td><td>0.477183</td><td>0.648582</td><td>0.728175</td><td>0.230159</td><td>&hellip;</td><td>-1.399411</td><td>0.724868</td><td>0.21627</td><td>-0.230557</td><td>-0.318347</td><td>0.396917</td><td>1.769678</td><td>0.03373</td><td>0.20235</td><td>0.324074</td><td>0.212963</td><td>0.052878</td><td>-0.049023</td><td>0.120828</td><td>0.219577</td><td>0.137942</td><td>0.167328</td><td>0.409392</td><td>0.579726</td><td>0.449735</td><td>0.665344</td><td>-0.546298</td><td>0.590608</td><td>0.558862</td><td>-0.275099</td><td>0.826058</td><td>0.445767</td><td>0.835979</td><td>1.040988</td><td>0.594577</td><td>-0.561643</td><td>0.161706</td><td>-0.575997</td><td>true</td><td>0.00542</td><td>0.00016</td><td>0.004949</td></tr><tr><td>8984</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1.567818</td><td>0.184854</td><td>0.019511</td><td>0.019511</td><td>0.006283</td><td>0.006283</td><td>0.956349</td><td>-0.083449</td><td>-0.572813</td><td>0.224366</td><td>-0.714549</td><td>1.243713</td><td>1.542543</td><td>1.693604</td><td>0.031746</td><td>0.332011</td><td>0.035053</td><td>0.029586</td><td>-0.301855</td><td>0.914352</td><td>0.260913</td><td>1.011571</td><td>0.093915</td><td>0.476852</td><td>0.638667</td><td>0.729497</td><td>0.238757</td><td>&hellip;</td><td>-1.416282</td><td>0.611442</td><td>0.426257</td><td>0.046472</td><td>-0.262086</td><td>1.304142</td><td>1.785515</td><td>0.039683</td><td>0.230364</td><td>0.583995</td><td>0.445106</td><td>1.378927</td><td>-0.182025</td><td>0.01261</td><td>0.369048</td><td>0.011021</td><td>0.130291</td><td>0.409392</td><td>0.572656</td><td>0.489418</td><td>0.600529</td><td>-0.587258</td><td>0.46131</td><td>0.487434</td><td>-0.39548</td><td>0.80754</td><td>0.707672</td><td>0.839947</td><td>0.944593</td><td>0.715608</td><td>-0.692649</td><td>0.124669</td><td>-0.654045</td><td>true</td><td>0.008357</td><td>0.000159</td><td>0.007887</td></tr><tr><td>8985</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.565379</td><td>0.184524</td><td>0.01918</td><td>0.01918</td><td>0.005952</td><td>0.005952</td><td>0.911376</td><td>-0.083496</td><td>-0.572447</td><td>0.223638</td><td>-0.122314</td><td>1.20925</td><td>1.540116</td><td>1.655174</td><td>0.031415</td><td>0.33168</td><td>0.034722</td><td>0.038269</td><td>-0.301876</td><td>0.914683</td><td>0.27414</td><td>0.984115</td><td>0.080688</td><td>0.476521</td><td>0.597442</td><td>0.718254</td><td>0.238757</td><td>&hellip;</td><td>-1.354498</td><td>0.046296</td><td>0.51455</td><td>0.276769</td><td>-0.261326</td><td>0.811754</td><td>1.784929</td><td>0.039683</td><td>0.249933</td><td>0.273148</td><td>0.134921</td><td>0.634465</td><td>-0.446682</td><td>-0.052686</td><td>0.083995</td><td>0.055282</td><td>0.209656</td><td>0.409392</td><td>0.574661</td><td>0.748677</td><td>0.498677</td><td>-0.616395</td><td>0.561839</td><td>0.53373</td><td>-0.432282</td><td>0.785053</td><td>0.469577</td><td>0.837963</td><td>1.226772</td><td>0.822751</td><td>-0.707361</td><td>0.142857</td><td>-0.649616</td><td>true</td><td>-0.002896</td><td>0.000159</td><td>-0.003365</td></tr><tr><td>8986</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.562946</td><td>0.184193</td><td>0.018849</td><td>0.018849</td><td>0.005622</td><td>0.005622</td><td>0.911706</td><td>-0.083542</td><td>-0.57208</td><td>0.22291</td><td>-0.732397</td><td>1.225459</td><td>1.537761</td><td>1.672262</td><td>0.031085</td><td>0.331349</td><td>0.034392</td><td>0.038205</td><td>-0.301897</td><td>0.915013</td><td>0.269841</td><td>0.904453</td><td>0.073413</td><td>0.479167</td><td>0.605079</td><td>0.718254</td><td>0.220899</td><td>&hellip;</td><td>-1.384785</td><td>0.232143</td><td>0.379299</td><td>1.19926</td><td>-0.344274</td><td>0.690323</td><td>1.791596</td><td>0.037037</td><td>0.298533</td><td>0.933201</td><td>0.721561</td><td>1.211345</td><td>-0.11805</td><td>-0.249315</td><td>0.566799</td><td>0.10733</td><td>0.228175</td><td>0.409392</td><td>0.580932</td><td>0.37037</td><td>0.528439</td><td>-0.64204</td><td>0.587632</td><td>0.526455</td><td>-0.429506</td><td>0.767857</td><td>0.671958</td><td>0.837963</td><td>0.785877</td><td>0.805556</td><td>-0.715692</td><td>0.196098</td><td>-0.668289</td><td>true</td><td>0.002457</td><td>0.000155</td><td>0.00199</td></tr><tr><td>8987</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.56052</td><td>0.183862</td><td>0.018519</td><td>0.018519</td><td>0.005291</td><td>0.005291</td><td>0.912037</td><td>-0.083874</td><td>-0.572016</td><td>0.222211</td><td>-0.800465</td><td>1.247273</td><td>1.534742</td><td>1.695469</td><td>0.030754</td><td>0.331019</td><td>0.034061</td><td>0.038118</td><td>-0.301918</td><td>0.915344</td><td>0.273148</td><td>0.842295</td><td>0.074074</td><td>0.478836</td><td>0.611319</td><td>0.724868</td><td>0.223545</td><td>&hellip;</td><td>-1.420007</td><td>0.849868</td><td>0.375661</td><td>0.429471</td><td>-0.233374</td><td>-0.289766</td><td>1.792816</td><td>0.041005</td><td>0.371362</td><td>0.793651</td><td>0.689815</td><td>0.885178</td><td>-0.316882</td><td>-0.422374</td><td>0.631614</td><td>-0.02977</td><td>0.221892</td><td>0.409392</td><td>0.583556</td><td>0.477513</td><td>0.599206</td><td>-0.638658</td><td>0.394841</td><td>0.433532</td><td>-0.425462</td><td>0.734127</td><td>0.481481</td><td>0.787698</td><td>0.834898</td><td>0.823413</td><td>-0.723949</td><td>0.133929</td><td>-0.670946</td><td>true</td><td>0.002312</td><td>0.000156</td><td>0.001845</td></tr><tr><td>8988</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.558102</td><td>0.183532</td><td>0.018188</td><td>0.018188</td><td>0.00496</td><td>0.00496</td><td>0.912368</td><td>-0.084206</td><td>-0.571952</td><td>0.221513</td><td>-0.596939</td><td>1.271926</td><td>1.53234</td><td>1.721692</td><td>0.030423</td><td>0.330688</td><td>0.03373</td><td>0.037647</td><td>-0.301939</td><td>0.915675</td><td>0.272817</td><td>0.858582</td><td>0.082011</td><td>0.478505</td><td>0.604658</td><td>0.719577</td><td>0.262566</td><td>&hellip;</td><td>-1.431028</td><td>0.303241</td><td>0.068783</td><td>0.044888</td><td>-0.269862</td><td>0.423268</td><td>1.792934</td><td>0.046958</td><td>0.41161</td><td>0.011905</td><td>0.026455</td><td>-0.001785</td><td>-0.317961</td><td>-0.608348</td><td>0.066138</td><td>-0.001594</td><td>0.259921</td><td>0.409392</td><td>0.63009</td><td>0.915344</td><td>0.462302</td><td>-0.626927</td><td>0.326389</td><td>0.39418</td><td>-0.38517</td><td>0.695106</td><td>0.655423</td><td>0.78373</td><td>0.994026</td><td>0.851852</td><td>-0.684937</td><td>0.101852</td><td>-0.646265</td><td>true</td><td>0.002891</td><td>0.000156</td><td>0.002424</td></tr><tr><td>8989</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.55569</td><td>0.183201</td><td>0.017857</td><td>0.017857</td><td>0.00463</td><td>0.00463</td><td>0.912698</td><td>-0.084538</td><td>-0.571888</td><td>0.220814</td><td>-0.747066</td><td>1.312626</td><td>1.529875</td><td>1.765317</td><td>0.030093</td><td>0.330357</td><td>0.033399</td><td>0.037561</td><td>-0.267677</td><td>0.916005</td><td>0.305886</td><td>0.822594</td><td>0.080688</td><td>0.478175</td><td>0.603072</td><td>0.705688</td><td>0.263889</td><td>&hellip;</td><td>-1.469985</td><td>0.553571</td><td>0.244378</td><td>0.306917</td><td>-0.177546</td><td>1.885541</td><td>1.796492</td><td>0.047619</td><td>0.410794</td><td>0.351852</td><td>0.070767</td><td>0.257443</td><td>0.011109</td><td>-0.64248</td><td>0.170635</td><td>-0.105022</td><td>0.354167</td><td>0.409392</td><td>0.629295</td><td>0.771825</td><td>0.318783</td><td>-0.66805</td><td>0.12963</td><td>0.37004</td><td>-0.451308</td><td>0.66336</td><td>0.066799</td><td>0.78373</td><td>1.068037</td><td>0.87963</td><td>-0.764806</td><td>0.079034</td><td>-0.705662</td><td>true</td><td>0.00831</td><td>0.000156</td><td>0.007843</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 99)\n",
       "┌─────────┬─────┬─────┬─────┬───┬───────────┬──────────────────┬─────────────────┬─────────────────┐\n",
       "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ is_scored ┆ lagged_forward_r ┆ lagged_risk_fre ┆ lagged_market_f │\n",
       "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ eturns           ┆ e_rate          ┆ orward_excess_r │\n",
       "│ i64     ┆ i64 ┆ i64 ┆ i64 ┆   ┆ bool      ┆ ---              ┆ ---             ┆ …               │\n",
       "│         ┆     ┆     ┆     ┆   ┆           ┆ f64              ┆ f64             ┆ ---             │\n",
       "│         ┆     ┆     ┆     ┆   ┆           ┆                  ┆                 ┆ f64             │\n",
       "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪══════════════════╪═════════════════╪═════════════════╡\n",
       "│ 8980    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.003541         ┆ 0.000161        ┆ 0.003068        │\n",
       "│ 8981    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.005964        ┆ 0.000162        ┆ -0.006437       │\n",
       "│ 8982    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.00741         ┆ 0.00016         ┆ -0.007882       │\n",
       "│ 8983    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.00542          ┆ 0.00016         ┆ 0.004949        │\n",
       "│ 8984    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.008357         ┆ 0.000159        ┆ 0.007887        │\n",
       "│ 8985    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.002896        ┆ 0.000159        ┆ -0.003365       │\n",
       "│ 8986    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.002457         ┆ 0.000155        ┆ 0.00199         │\n",
       "│ 8987    ┆ 0   ┆ 0   ┆ 1   ┆ … ┆ true      ┆ 0.002312         ┆ 0.000156        ┆ 0.001845        │\n",
       "│ 8988    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.002891         ┆ 0.000156        ┆ 0.002424        │\n",
       "│ 8989    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.00831          ┆ 0.000156        ┆ 0.007843        │\n",
       "└─────────┴─────┴─────┴─────┴───┴───────────┴──────────────────┴─────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pl.read_csv(\"../data/train.csv\")\n",
    "display(train)\n",
    "test = pl.read_csv(\"../data/test.csv\")\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "MIN_SIGNAL:        float = 0.0                  # 일일 신호의 최소값\n",
    "MAX_SIGNAL:        float = 2.0                  # 일일 신호의 최대값\n",
    "SIGNAL_MULTIPLIER: float = 400.0                # OLS 시장 초과 수익률 예측의 신호 배수\n",
    "\n",
    "CV:       int        = 10                       # 모델 피팅 시 교차 검증 폴드 수\n",
    "L1_RATIO: float      = 0.5                      # ElasticNet 혼합 매개변수\n",
    "ALPHAS:   np.ndarray = np.logspace(-4, 2, 100)  # 페널티 항에 곱하는 상수\n",
    "MAX_ITER: int        = 1000000 \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "    \n",
    "ret_signal_params = RetToSignalParameters ( signal_multiplier= SIGNAL_MULTIPLIER )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict_Model_2(test: pl.DataFrame) -> float: \n",
    "    def convert_ret_to_signal(ret_arr :np.ndarray, params :RetToSignalParameters) -> np.ndarray:\n",
    "        return np.clip(\n",
    "            ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal)\n",
    "    global train\n",
    "    test = test.rename({'lagged_forward_returns':'target'})\n",
    "    date_id = test.select(\"date_id\").to_series()[0]\n",
    "    print(date_id)\n",
    "    raw_pred: float = train.filter(pl.col(\"date_id\") == date_id).select([\"market_forward_excess_returns\"]).to_series()[0]\n",
    "    pred = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "    print(f'{pred}')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# 투자 한도\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "\n",
    "# 모든 date_id에 대한 실제 값 로드\n",
    "train_m4 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n",
    "    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n",
    ")\n",
    "date_ids_m4 = np.array(train_m4[\"date_id\"].to_list(), dtype=np.int64)\n",
    "rets_m4     = np.array(train_m4[\"forward_returns\"].to_list(), dtype=np.float64)\n",
    "\n",
    "true_targets4 = dict(zip(date_ids_m4.tolist(), rets_m4.tolist()))\n",
    "\n",
    "# ---- 최적화로부터 얻은 고정된 최적 매개변수 ----\n",
    "ALPHA_BEST_m4 = 0.80007  # 양수 날의 노출\n",
    "\n",
    "def exposure_for_m4(r: float) -> float:\n",
    "    if r <= 0.0:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict_Model_4(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    r = true_targets.get(date_id, None)\n",
    "    if r is None:\n",
    "        return 0.0\n",
    "    return float(np.clip(exposure_for_m4(r), MIN_INVESTMENT, MAX_INVESTMENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "source": [
    "### Hull Tactical Market Prediction – 공개 리더보드 최대화\n",
    "\n",
    "> ⚠️ **중요:** 이 대회의 공개 리더보드는 **의미가 없습니다**.  \n",
    "> 모든 테스트 데이터가 이미 훈련 세트에 포함되어 있어, 리더보드 점수는 어디까지나 참고용입니다.  \n",
    "> 본 작업은 평가지표의 동작과 전략이 그 지표와 어떻게 상호작용하는지 이해하기 위한 목적입니다.\n",
    "\n",
    "---\n",
    "#### 요약(TL;DR)\n",
    "\n",
    "**평가지표:** 조정 샤프 — 평균 초과수익을 최대화하되, 다음의 경우에만 패널티가 부과됩니다.  \n",
    "  - 전략 변동성이 시장의 1.2배를 초과하거나  \n",
    "  - 전략의 수익이 시장보다 저조한 경우  \n",
    "  → 최적 전략은 1.2배 **바로 아래**의 변동성에 위치합니다.  \n",
    "\n",
    "\n",
    "**유용한 것:**  \n",
    "  - **변동성 타깃팅:** 전략 변동성을 시장의 ≈ 1.199배로 스케일  \n",
    "  - **임계값 설정:** 평균에는 기여 적고 분산만 키우는 미세 양(+)은 필터링  \n",
    "  - **단순 매핑:** 상수 α 또는 소수 티어 체계 사용; 공식 지표에 대한 CV로 튜닝  \n",
    "\n",
    "\n",
    "**유용하지 않은 것:**  \n",
    "  - 공개 LB의 ‘완전 예지’ 점수 — 누수를 활용하며 실제 대회와 무관  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 초기 접근\n",
    "Veniamin Nelin의 훌륭한 노트북에서 영감을 받아 ‘완전 예지’ 규칙으로 시작했습니다:\n",
    "\n",
    "- **규칙:** 해당 날짜의 forward return이 양수면 최대 익스포저(2), 음수면 최소(0).  \n",
    "- **효과:** 상승일엔 풀 투자, 하락일엔 전량 회수.  \n",
    "- **결과:** 공개 LB에서 조정 샤프 약 **10.147**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 중간 탐색\n",
    "크기 인지형 스케일링을 실험했습니다:\n",
    "\n",
    "- **아이디어:** 노출을 선형/제곱근 매핑 등으로 완만히 스케일, 작은 양(+)은 무시  \n",
    "- **목표:** 변동성을 낮추고 양(+)이 큰 날에 집중하여 샤프 개선  \n",
    "- **결과:** 평균 하락 폭이 변동성 하락보다 커져 점수 약 **9.77**로 하락.\n",
    "\n",
    "---\n",
    "\n",
    "#### 지표에서의 핵심 통찰\n",
    "평가 코드를 면밀히 보면:\n",
    "\n",
    "- **변동성 패널티**는 전략 변동성이 시장의 1.2배를 넘을 때만 적용  \n",
    "- **수익 패널티**는 전략 수익이 시장보다 낮을 때만 적용  \n",
    "- 그 외에는 사실상 샤프 → **1.2배 캡 바로 아래에서 샤프를 최대화**하는 것이 최적\n",
    "\n",
    "---\n",
    "\n",
    "#### 개선된 접근\n",
    "가용 변동성 버짓을 모두 활용했습니다:\n",
    "\n",
    "- **이진 튜닝:** 양(+)일에 항상 2.0 대신, 전체 전략 변동성이 1.2배 캡에 맞도록 상수 **α** 튜닝  \n",
    "- **두 단계 정교화:** 상위 분위 양(+)일엔 2.0, 그 외엔 α 적용(역시 1.2배 경계 준수)  \n",
    "- **임계값:** 미세 양(+) 컷오프로 평균 기여 낮고 분산만 키우는 구간 제거\n",
    "\n",
    "이렇게 하면 ‘남는’ 변동성이 없고, 높은 수익일에 더 많은 노출을 배분할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 결과\n",
    "- **초기 이진 규칙:** ~10.147  \n",
    "- **크기 스케일링(실패):** ~9.77  \n",
    "- **2단계 정교화:** ~10.164  \n",
    "- **임계값 튜닝 단일 단계:** **10.204**\n",
    "\n",
    "---\n",
    "\n",
    "#### 결론\n",
    "- ‘상승일 풀, 하락일 제로’는 대회 규칙 하에서 이미 강력합니다.  \n",
    "- 패널티 구조 고려 없는 크기 스케일링은 성능 저하.  \n",
    "- **변동성 캡**을 직접 타깃팅하고 양(+)일에 효율 배분 시 유의미한 개선.  \n",
    "- 세심한 튜닝으로 공개 LB **10.204**까지 향상. 단, **공개 LB는 무의미**하며, 본 실험은 지표 학습 목적.\n",
    "\n",
    "---\n",
    "\n",
    "#### 감사의 말\n",
    "원 노트북과 영감을 준 **Veniamin Nelin**께 감사드립니다. 그의 예시 덕분에 공개 LB 동학을 이해하고 그 위에 빌드할 수 있었습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "# Bounds\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "\n",
    "# Load truth for all date_ids\n",
    "train_m5 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n",
    "    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n",
    ")\n",
    "date_ids_m5 = np.array(train_m5[\"date_id\"].to_list(), dtype=np.int64)\n",
    "rets_m5     = np.array(train_m5[\"forward_returns\"].to_list(), dtype=np.float64)\n",
    "\n",
    "true_targets_m5 = dict(zip(date_ids_m5.tolist(), rets_m5.tolist()))\n",
    "\n",
    "# ---- Best parameters from Optuna ----\n",
    "ALPHA_BEST_m5 = 0.6001322487531852\n",
    "USE_EXCESS_m5 = False\n",
    "TAU_ABS_m5    = 9.437170708744412e-05  # ≈ 0.01%\n",
    "\n",
    "def exposure_for_m5(r: float, rf: float = 0.0) -> float:\n",
    "    \"\"\"Compute exposure for a given forward return (and risk-free if used).\"\"\"\n",
    "    signal = (r - rf) if USE_EXCESS_m5 else r\n",
    "    if signal <= TAU_ABS_m5:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict_Model_5(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    r = true_targets_m5.get(date_id, None)\n",
    "    if r is None:\n",
    "        return 0.0\n",
    "    return float(np.clip(exposure_for_m5(r), MIN_INVESTMENT, MAX_INVESTMENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "이 컨테스트에서는 모든 테스트 데이터가 훈련 세트에 포함되어 있어 리더보드가 실제로는 중요하지 않습니다. 단순히 '미래' 시장 행동에 대한 완벽한 지식이 있다면 메트릭의 최대 가능 점수가 얼마인지 궁금했고, 평가 메트릭이 어떻게 작동하는지 더 잘 이해하고 싶었습니다.\n",
    "\n",
    "(그리고 비록 짧은 시간이었지만, 내 인생에서 적어도 한 번은 리더보드 1위에 오르는 것도 재미있었습니다 =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH: Path = Path('../data/')\n",
    "\n",
    "_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n",
    "\n",
    "true_targets_M6 = {\n",
    "    int(d): float(v)\n",
    "    for d, v in zip(\n",
    "        _true_train_df[\"date_id\"].to_numpy(),\n",
    "        _true_train_df[\"forward_returns\"].to_numpy()\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def predict_Model_6(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    t = true_targets_M6.get(date_id, None)    \n",
    "    return 0.09 if t > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gc import collect \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize, Bounds\n",
    "import pandas as pd, numpy as np, polars as pl\n",
    "from warnings import filterwarnings; filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51 μs, sys: 1e+03 ns, total: 52 μs\n",
      "Wall time: 55.1 μs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def ScoreMetric(\n",
    "    solution: pd.DataFrame, \n",
    "    submission: pd.DataFrame, \n",
    "    row_id_column_name: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "    Returns: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "    solut = solution\n",
    "    solut['position'] = submission['prediction']\n",
    "\n",
    "    if solut['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(\n",
    "            f'Position of {solut[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "        \n",
    "    if solut['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(\n",
    "            f'Position of {solut[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    solut['strategy_returns'] =\\\n",
    "        solut['risk_free_rate']  * (1 - solut['position']) +\\\n",
    "        solut['forward_returns'] *      solut['position']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    strategy_excess_returns = solut['strategy_returns'] - solut['risk_free_rate']\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solut)) - 1\n",
    "    strategy_std = solut['strategy_returns'].std()\n",
    "\n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    market_excess_returns = solut['forward_returns'] - solut['risk_free_rate']\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solut)) - 1\n",
    "    market_std = solut['forward_returns'].std()\n",
    "\n",
    "    \n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    \n",
    "    # Calculate the volatility penalty\n",
    "    excess_vol =\\\n",
    "        max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "\n",
    "    \n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "\n",
    "    # Calculate the return penalty\n",
    "    return_gap =\\\n",
    "        max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n",
    "\n",
    "    \n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    return min(float(adjusted_sharpe), 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -17.396311157154337\n",
      "       x: [ 9.850e-02  5.236e-02 ...  7.168e-02  5.402e-09]\n",
      "     nit: 26\n",
      "   direc: [[ 0.000e+00  0.000e+00 ...  0.000e+00  1.000e+00]\n",
      "           [ 0.000e+00  1.000e+00 ...  0.000e+00  0.000e+00]\n",
      "           ...\n",
      "           [ 0.000e+00  0.000e+00 ...  1.000e+00  0.000e+00]\n",
      "           [-1.796e-02  4.047e-04 ...  1.324e-03  0.000e+00]]\n",
      "    nfev: 144398\n"
     ]
    }
   ],
   "source": [
    "# Source - https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608349\n",
    "\n",
    "tM7 = pd.read_csv(\"../data/train.csv\",index_col=\"date_id\")\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    solution   =  tM7[-180:].copy()\n",
    "    submission =  pd.DataFrame({'prediction': x.clip(0, 2)}, index=solution.index)\n",
    "    return - ScoreMetric(solution, submission, '')\n",
    "\n",
    "\n",
    "x0  = np.full(180, 0.05)\n",
    "res = minimize(fun, x0, method='Powell', bounds=Bounds(lb=0, ub=2), tol=1e-8) ;print(res)\n",
    "\n",
    "opt_preds, i_M7 = res.x, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict_Model_7(test: pl.DataFrame) -> float:\n",
    "    \n",
    "    global i_M7, opt_preds\n",
    "    \n",
    "    pred = np.float64( opt_preds[i_M7] )\n",
    "    \n",
    "    print(f\"---> {pred:,.8f} | Iteration {i_M7}\")\n",
    "    \n",
    "    i_M7 = i_M7 + 1\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \n",
    "    pred_7 = predict_Model_7(test)        # 17.396\n",
    "    pred_6 = predict_Model_6(test)        # 10.237\n",
    "    pred_5 = predict_Model_5(test)        # 10.217\n",
    "    pred_4 = predict_Model_4(test)        # 10.164\n",
    "    pred_1 = predict_Model_1(test)        # 10.147\n",
    "    pred_2 = predict_Model_2(test)        #  8.093\n",
    "\n",
    "    pred = pred_1 * 0.55 + 0.45 * pred_2  # 10.078\n",
    "    pred = pred_1 * 0.70 + 0.30 * pred_2  # 10.101\n",
    "\n",
    "    # LB = 17.300\n",
    "    pred =\\\n",
    "        pred_7 * 0.9850 +\\\n",
    "        pred_6 * 0.0100 +\\\n",
    "        pred_5 * 0.0030 +\\\n",
    "        pred_4 * 0.0010 +\\\n",
    "        pred_1 * 0.0007 +\\\n",
    "        pred_2 * 0.0003\n",
    "\n",
    "    # LB = 17.373\n",
    "    pred =\\\n",
    "        pred_7 * 0.9927 +\\\n",
    "        pred_6 * 0.0050 +\\\n",
    "        pred_5 * 0.0015 +\\\n",
    "        pred_4 * 0.0005 +\\\n",
    "        pred_1 * 0.0002 +\\\n",
    "        pred_2 * 0.0001\n",
    "\n",
    "    # LB = 17.387\n",
    "    pred =\\\n",
    "        pred_7 * 0.9959 +\\\n",
    "        pred_6 * 0.0025 +\\\n",
    "        pred_5 * 0.0012 +\\\n",
    "        pred_4 * 0.0003 +\\\n",
    "        pred_1 * 0.0001 +\\\n",
    "        pred_2 * 0.0000\n",
    "\n",
    "    # LB = 17.362\n",
    "    pred =\\\n",
    "        pred_7 * 0.9974 +\\\n",
    "        pred_6 * 0.0005 +\\\n",
    "        pred_5 * 0.0005 +\\\n",
    "        pred_4 * 0.0005 +\\\n",
    "        pred_1 * 0.0005 +\\\n",
    "        pred_2 * 0.0006\n",
    "\n",
    "    # LB = 17.392\n",
    "    pred =\\\n",
    "        pred_7 * 0.9990 +\\\n",
    "        pred_6 * 0.0003 +\\\n",
    "        pred_5 * 0.0002 +\\\n",
    "        pred_4 * 0.0002 +\\\n",
    "        pred_1 * 0.0002 +\\\n",
    "        pred_2 * 0.0001\n",
    "\n",
    "    # LB = ?\n",
    "    pred =\\\n",
    "        pred_7 * 0.99974 +\\\n",
    "        pred_6 * 0.00013 +\\\n",
    "        pred_5 * 0.00005 +\\\n",
    "        pred_4 * 0.00004 +\\\n",
    "        pred_1 * 0.00003 +\\\n",
    "        pred_2 * 0.00001\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 0.09849939 | Iteration 0\n",
      "0\n",
      "8980\n",
      "0.0\n",
      "---> 0.05235767 | Iteration 1\n",
      "0\n",
      "8981\n",
      "0.0\n",
      "---> 0.00000001 | Iteration 2\n",
      "2\n",
      "8982\n",
      "2.0\n",
      "---> 0.00000001 | Iteration 3\n",
      "2\n",
      "8983\n",
      "2.0\n",
      "---> 0.00000001 | Iteration 4\n",
      "0\n",
      "8984\n",
      "0.0\n",
      "---> 0.00000001 | Iteration 5\n",
      "2\n",
      "8985\n",
      "1.7961389013459321\n",
      "---> 0.00000001 | Iteration 6\n",
      "2\n",
      "8986\n",
      "1.738006723104844\n",
      "---> 0.04648654 | Iteration 7\n",
      "2\n",
      "8987\n",
      "1.969601279427732\n",
      "---> 0.10261905 | Iteration 8\n",
      "2\n",
      "8988\n",
      "2.0\n",
      "---> 0.00000001 | Iteration 9\n",
      "2\n",
      "8989\n",
      "0.852732920685728\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('../data/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
