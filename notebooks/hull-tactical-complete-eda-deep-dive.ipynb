{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hull Tactical · Market Prediction\n\nThis competition, hosted by Hull Tactical, challenges participants to **predict the daily forward returns of the S&P 500 index**.  \nThe dataset provides decades of market information across multiple factor families:\n\n- **M\\*** — Market dynamics and technical features  \n- **E\\*** — Macro-economic indicators  \n- **I\\*** — Interest rate features  \n- **P\\*** — Price and valuation metrics  \n- **V\\*** — Volatility measures  \n- **S\\*** — Sentiment variables  \n- **MOM\\*** — Momentum-based indicators  \n- **D\\*** — Dummy/binary features  \n\nThe target variable is `forward_returns`, representing the next-day return from buying the S&P 500 and selling one day later.  \nAdditional columns like `risk_free_rate` and `market_forward_excess_returns` are included to capture relative performance against expectations.\n\nThe competition proceeds in **two phases**:\n1. **Training phase** — model development using historical data. Public leaderboard here is **not meaningful**, since it’s based on a mock test slice.  \n2. **Forecasting phase** — evaluation on truly unseen, future data via the Kaggle evaluation API. Final leaderboard scores will be based on this phase.\n\nThe goal is to build robust, generalizable models that can extract signal from noisy financial data while avoiding overfitting to the training set.\n","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries and setting configurations\nWe begin by loading the essential Python libraries for data handling, visualization, and feature analysis.  \n- **pandas / numpy** for data wrangling and numerical operations  \n- **matplotlib / seaborn / plotly** for static and interactive visualizations  \n- **scipy.stats** for statistical tests (e.g., normality checks)  \n- **scikit-learn tools** like `StandardScaler`, `PCA`, and `mutual_info_regression` for preprocessing and feature analysis  \n\nWe also configure warnings and set default plotting styles to keep our notebook outputs clean and consistent.\n","metadata":{}},{"cell_type":"code","source":"# import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime, timedelta\nfrom scipy import stats\nfrom scipy.stats import jarque_bera, shapiro\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\n# Configuration\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:32.583411Z","iopub.execute_input":"2025-09-17T17:29:32.583779Z","iopub.status.idle":"2025-09-17T17:29:37.766396Z","shell.execute_reply.started":"2025-09-17T17:29:32.583745Z","shell.execute_reply":"2025-09-17T17:29:37.765233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. DATA LOADING AND INITIAL INSPECTION\n\nWe start by reading the training and test datasets provided by the competition.  \nKey steps in this section include:\n- Checking the **shape** of both datasets and the **date range** covered in the training data.  \n- Summarizing **memory usage** and **data types** to understand dataset size and composition.  \n- Displaying the **first few rows** to get a quick sense of the feature structure (M*, E*, I*, P*, V*, S*, MOM*, D*).  \n- Verifying if there are any **duplicate rows** in the training set.  \n\nThis initial inspection helps us confirm the integrity of the data and sets the stage for deeper exploratory analysis.\n","metadata":{}},{"cell_type":"code","source":"def load_and_inspect_data():\n    \"\"\"Load and perform initial inspection of the dataset\"\"\"\n    \n    print(\"🔍 LOADING AND INSPECTING DATA\")\n    print(\"=\"*50)\n    \n    # Load datasets\n    train_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\n    test_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n    \n    print(f\"📊 Training set shape: {train_df.shape}\")\n    print(f\"📊 Test set shape: {test_df.shape}\")\n    print(f\"📅 Date range: {train_df['date_id'].min()} to {train_df['date_id'].max()}\")\n    \n    # Basic info about the datasets\n    print(\"\\n📋 TRAINING SET INFO:\")\n    print(f\"Memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    print(f\"Data types distribution:\")\n    print(train_df.dtypes.value_counts())\n    \n    # Display first few rows\n    print(\"\\n🔍 First 5 rows of training data:\")\n    display(train_df.head())\n    \n    # Check for duplicates\n    duplicates = train_df.duplicated().sum()\n    print(f\"\\n🔍 Duplicate rows in training set: {duplicates}\")\n    \n    return train_df, test_df\n\ntrain_df, test_df = load_and_inspect_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:37.768121Z","iopub.execute_input":"2025-09-17T17:29:37.768657Z","iopub.status.idle":"2025-09-17T17:29:38.189245Z","shell.execute_reply.started":"2025-09-17T17:29:37.768631Z","shell.execute_reply":"2025-09-17T17:29:38.188121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. MISSING VALUES ANALYSIS\n\nUnderstanding the structure of missing data is crucial in financial time-series modeling. This function provides a **comprehensive overview of missing values** in the dataset:  \n\n1. **Overall Statistics**  \n   - Total missing values across the dataset.  \n   - Percentage of missing entries relative to the entire data matrix.  \n\n2. **Column-Level Missingness**  \n   - Displays the top 20 columns with the most missing values.  \n   - Helps identify whether certain features are systematically sparse.  \n\n3. **Visual Diagnostics**  \n   - **Heatmap (first 50 columns):** Provides a bird’s-eye view of missing patterns across features. Vertical bands may indicate entire columns with missing values, while horizontal bands may point to specific dates or securities affected.  \n   - **Bar Chart of Top 20 Missing Columns:** Highlights which features are the most incomplete, helping decide whether to drop or impute them.  \n   - **Missing Values Over Time (Line Plot):** Shows how missingness evolves across `date_id`. Spikes may correspond to market holidays, data outages, or structural changes in the dataset.  \n   - **Missing Percentage Over Time:** Complements the count view, showing the proportion of missing values per date, which helps normalize by dataset size. \n","metadata":{}},{"cell_type":"code","source":"def analyze_missing_values(df):\n    \"\"\"Comprehensive missing values analysis\"\"\"\n    \n    print(\"\\n🕳️ MISSING VALUES ANALYSIS\")\n    print(\"=\"*50)\n    \n    # Overall missing statistics\n    total_cells = df.shape[0] * df.shape[1]\n    total_missing = df.isnull().sum().sum()\n    missing_percentage = (total_missing / total_cells) * 100\n    \n    print(f\"Total missing values: {total_missing:,} ({missing_percentage:.2f}% of all cells)\")\n    \n    # Missing values per column\n    missing_data = df.isnull().sum().sort_values(ascending=False)\n    missing_data = missing_data[missing_data > 0]\n    \n    if len(missing_data) > 0:\n        missing_percent = (missing_data / len(df)) * 100\n        missing_df = pd.DataFrame({\n            'Missing_Count': missing_data,\n            'Missing_Percentage': missing_percent\n        })\n        \n        print(f\"\\n📊 Top 20 columns with missing values:\")\n        display(missing_df.head(20))\n        \n        # Visualize missing values pattern\n        if len(missing_data) > 0:\n            fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n            \n            # Missing values heatmap\n            missing_matrix = df.isnull()\n            sns.heatmap(missing_matrix.iloc[:, :50], cbar=True, ax=axes[0,0], cmap='viridis')\n            axes[0,0].set_title('Missing Values Heatmap (First 50 Columns)')\n            \n            # Missing values bar chart\n            top_missing = missing_df.head(20)\n            top_missing['Missing_Percentage'].plot(kind='bar', ax=axes[0,1])\n            axes[0,1].set_title('Top 20 Columns by Missing Percentage')\n            axes[0,1].set_ylabel('Missing Percentage (%)')\n            plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45)\n            \n            # Missing values over time\n            if 'date_id' in df.columns:\n                missing_by_date = df.groupby('date_id').apply(lambda x: x.isnull().sum().sum())\n                axes[1,0].plot(missing_by_date.index, missing_by_date.values)\n                axes[1,0].set_title('Missing Values Over Time')\n                axes[1,0].set_xlabel('Date ID')\n                axes[1,0].set_ylabel('Total Missing Values')\n                \n                # Missing percentage by date\n                missing_pct_by_date = (missing_by_date / df.shape[1]) * 100\n                axes[1,1].plot(missing_pct_by_date.index, missing_pct_by_date.values)\n                axes[1,1].set_title('Missing Values Percentage Over Time')\n                axes[1,1].set_xlabel('Date ID')\n                axes[1,1].set_ylabel('Missing Percentage (%)')\n            \n            plt.tight_layout()\n            plt.show()\n    \n    return missing_data\n\nmissing_analysis = analyze_missing_values(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:38.190285Z","iopub.execute_input":"2025-09-17T17:29:38.190611Z","iopub.status.idle":"2025-09-17T17:29:43.211065Z","shell.execute_reply.started":"2025-09-17T17:29:38.190586Z","shell.execute_reply":"2025-09-17T17:29:43.209968Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🔎 Missing Values Findings  \n\n- **Overall Missingness:**  \n  The dataset has **137,675 missing values**, which accounts for **15.63% of all cells**. This is significant enough to influence model performance and requires a careful handling strategy.  \n\n- **Top Features with Missing Data:**  \n  The following columns show the highest degree of missingness:  \n  - `E7` → 77.5% missing  \n  - `V10` → 67.3% missing  \n  - `S3` → 63.8% missing  \n  - `M1`, `M13`, `M14` → ~61.6% missing each  \n\n  These features are candidates for **feature removal** (if deemed redundant) or **special imputation** (if domain knowledge suggests they hold critical information).  \n\n- **Moderately Affected Features (30–50% missing):**  \n  Columns like `M6`, `V9`, and `S12` fall into this range. They might still provide signal after thoughtful imputation, but could also add noise if handled poorly.  \n\n- **Lightly Affected Features (<25% missing):**  \n  Columns like `E1`, `P6`, `E20`, `P7`, `P5`, `V5`, and `S5` have missing rates between 16–22%. These are typically worth keeping, provided we apply a robust imputation strategy (median, forward-fill, or predictive imputation).  ","metadata":{}},{"cell_type":"markdown","source":"# 3. FEATURE CATEGORIZATION AND ANALYSIS","metadata":{}},{"cell_type":"code","source":"def categorize_features(df):\n    \"\"\"Categorize features based on their prefixes\"\"\"\n    \n    print(\"\\n🏷️ FEATURE CATEGORIZATION\")\n    print(\"=\"*50)\n    \n    feature_categories = {}\n    \n    # Define feature categories based on prefixes\n    categories = {\n        'Market_Dynamics': 'M',\n        'Macro_Economic': 'E',\n        'Interest_Rate': 'I',\n        'Price_Valuation': 'P',\n        'Volatility': 'V',\n        'Sentiment': 'S',\n        'Momentum': 'MOM',\n        'Dummy_Binary': 'D'\n    }\n    \n    # Categorize features\n    for category, prefix in categories.items():\n        if prefix == 'MOM':\n            feature_categories[category] = [col for col in df.columns if col.startswith(prefix)]\n        else:\n            feature_categories[category] = [col for col in df.columns if col.startswith(prefix) and not col.startswith('MOM')]\n    \n    # Special columns\n    special_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n    feature_categories['Special'] = [col for col in special_cols if col in df.columns]\n    \n    # Display categorization\n    total_features = 0\n    for category, features in feature_categories.items():\n        print(f\"📊 {category}: {len(features)} features\")\n        total_features += len(features)\n    \n    print(f\"\\n📈 Total categorized features: {total_features}\")\n    print(f\"📈 Total columns in dataset: {df.shape[1]}\")\n    \n    return feature_categories\n\nfeature_categories = categorize_features(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:43.212283Z","iopub.execute_input":"2025-09-17T17:29:43.212631Z","iopub.status.idle":"2025-09-17T17:29:43.223266Z","shell.execute_reply.started":"2025-09-17T17:29:43.212606Z","shell.execute_reply":"2025-09-17T17:29:43.221957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 📊 Categorization Results  \n\nAll **98 dataset features** were successfully mapped into domain-specific categories:  \n\n- **Market Dynamics (18 features):** Direct market indicators (spreads, liquidity, market internals).  \n- **Macro-Economic (20 features):** Broad economic signals — employment, inflation, growth proxies.  \n- **Interest Rate (9 features):** Yield curve and rate-sensitive variables.  \n- **Price Valuation (13 features):** Relative asset pricing & valuation ratios.  \n- **Volatility (13 features):** Realized/implied volatility measures, uncertainty indicators.  \n- **Sentiment (12 features):** Investor surveys, behavioral/psychological sentiment indicators.  \n- **Momentum (0 features):** None explicitly provided — could suggest Hull Tactical encodes these trends elsewhere.  \n- **Dummy/Binary (9 features):** Structural flags and categorical encodings.  \n- **Special (4 features):** Competition-specific columns: `date_id`, `forward_returns`, `risk_free_rate`, `market_forward_excess_returns`.  \n\n📌 **Key Insights:**  \n1. The dataset is **well-balanced across multiple financial dimensions**, with a healthy representation of macro, sentiment, and volatility.  \n2. **No momentum features** → This is unusual in financial modeling, where momentum is a strong predictive factor. It will be interesting to see whether other feature groups indirectly capture momentum.  \n3. **Special columns** clearly define the prediction target (`forward_returns`) and evaluation framework (market excess returns, risk-free rate).   \n","metadata":{}},{"cell_type":"markdown","source":"# 4. TARGET VARIABLE ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_target_variable(df):\n    \"\"\"Comprehensive analysis of target variables\"\"\"\n    \n    print(\"\\n🎯 TARGET VARIABLE ANALYSIS\")\n    print(\"=\"*50)\n    \n    target_cols = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n    target_cols = [col for col in target_cols if col in df.columns]\n    \n    fig, axes = plt.subplots(3, len(target_cols), figsize=(20, 15))\n    if len(target_cols) == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for i, col in enumerate(target_cols):\n        if col in df.columns:\n            data = df[col].dropna()\n            \n            print(f\"\\n📊 {col.upper()} Statistics:\")\n            print(f\"Count: {len(data):,}\")\n            print(f\"Mean: {data.mean():.6f}\")\n            print(f\"Std: {data.std():.6f}\")\n            print(f\"Min: {data.min():.6f}\")\n            print(f\"25%: {data.quantile(0.25):.6f}\")\n            print(f\"50%: {data.median():.6f}\")\n            print(f\"75%: {data.quantile(0.75):.6f}\")\n            print(f\"Max: {data.max():.6f}\")\n            print(f\"Skewness: {data.skew():.6f}\")\n            print(f\"Kurtosis: {data.kurtosis():.6f}\")\n            \n            # Distribution plot\n            axes[0, i].hist(data, bins=50, alpha=0.7, density=True)\n            axes[0, i].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.4f}')\n            axes[0, i].axvline(data.median(), color='green', linestyle='--', label=f'Median: {data.median():.4f}')\n            axes[0, i].set_title(f'{col} Distribution')\n            axes[0, i].legend()\n            \n            # Time series plot\n            if 'date_id' in df.columns:\n                axes[1, i].plot(df['date_id'], df[col], alpha=0.7)\n                axes[1, i].set_title(f'{col} Over Time')\n                axes[1, i].set_xlabel('Date ID')\n            \n            # Q-Q plot for normality check\n            stats.probplot(data, dist=\"norm\", plot=axes[2, i])\n            axes[2, i].set_title(f'{col} Q-Q Plot')\n            \n            # Statistical tests\n            jb_stat, jb_pvalue = jarque_bera(data)\n            print(f\"Jarque-Bera test p-value: {jb_pvalue:.6f} ({'Normal' if jb_pvalue > 0.05 else 'Not Normal'})\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return target_cols\n\ntarget_analysis = analyze_target_variable(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:43.225468Z","iopub.execute_input":"2025-09-17T17:29:43.225889Z","iopub.status.idle":"2025-09-17T17:29:45.66582Z","shell.execute_reply.started":"2025-09-17T17:29:43.225864Z","shell.execute_reply":"2025-09-17T17:29:45.664965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🎯 Target Variable Analysis  \n\nWe focus on the three key target-related columns in this dataset:  \n\n1. **forward_returns** → The next-day S&P 500 returns (our prediction target).  \n2. **risk_free_rate** → The federal funds rate, representing the baseline cost of capital.  \n3. **market_forward_excess_returns** → Returns relative to long-term expectations (adjusted via rolling mean & MAD winsorization).  \n\n#### Key Findings:  \n\n- **Forward Returns**  \n  - Distribution is centered near zero (mean ≈ 0.00047), as expected for daily returns.  \n  - Standard deviation ≈ 1%, showing typical daily market volatility.  \n  - Slight negative skewness (-0.18), meaning downside moves can be a bit heavier.  \n  - Kurtosis ≈ 2.19 → lighter tails compared to a normal distribution.  \n  - **Not normally distributed** (Jarque-Bera p < 0.05).  \n\n- **Risk-Free Rate**  \n  - Very small daily changes (mean ≈ 0.0001).  \n  - Negligible volatility relative to returns.  \n  - Negative kurtosis (-1.27) indicates thinner tails → distribution more “flat” than normal.  \n  - Also **not normal**, though deviations are less impactful.  \n\n- **Market Forward Excess Returns**  \n  - Similar distribution to `forward_returns` (mean ≈ 0.00005).  \n  - Std ≈ 1%, confirming it’s just a detrended view of raw returns.  \n  - Slight negative skewness (-0.18) and kurtosis ≈ 2.24.  \n  - **Not normal** by statistical tests.  \n\n#### Visual Takeaways:  \n- **Histograms** show heavy concentration around zero, but tails deviate from Gaussian.  \n- **Time-series plots** highlight volatility clustering (periods of calm vs turbulence).  \n- **Q-Q plots** confirm fat tails, a common feature in financial time series.  \n\n📌 **Implication for Modeling:**  \nLinear models assuming normality may struggle. We should consider:  \n- Robust loss functions (Huber, quantile regression).  \n- Non-linear models (tree-based, neural nets).  \n- Feature engineering around volatility and tail risk.  \n","metadata":{}},{"cell_type":"markdown","source":"# 5. FEATURE DISTRIBUTION ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_feature_distributions(df, feature_categories, sample_size=10):\n    \"\"\"Analyze distributions of different feature categories\"\"\"\n    \n    print(\"\\n📊 FEATURE DISTRIBUTION ANALYSIS\")\n    print(\"=\"*50)\n    \n    for category, features in feature_categories.items():\n        if category == 'Special' or len(features) == 0:\n            continue\n            \n        print(f\"\\n🔍 Analyzing {category} features ({len(features)} total)\")\n        \n        # Sample features for visualization\n        sample_features = features[:min(sample_size, len(features))]\n        \n        if len(sample_features) > 0:\n            # Create distribution plots\n            n_cols = min(4, len(sample_features))\n            n_rows = (len(sample_features) + n_cols - 1) // n_cols\n            \n            fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n            if n_rows == 1:\n                axes = axes.reshape(1, -1)\n            elif n_cols == 1:\n                axes = axes.reshape(-1, 1)\n            \n            for idx, feature in enumerate(sample_features):\n                row = idx // n_cols\n                col = idx % n_cols\n                \n                data = df[feature].dropna()\n                if len(data) > 0:\n                    axes[row, col].hist(data, bins=30, alpha=0.7)\n                    axes[row, col].set_title(f'{feature}')\n                    axes[row, col].axvline(data.mean(), color='red', linestyle='--', alpha=0.8)\n                    \n                    # Add statistics as text\n                    stats_text = f'Mean: {data.mean():.3f}\\nStd: {data.std():.3f}\\nSkew: {data.skew():.2f}'\n                    axes[row, col].text(0.02, 0.98, stats_text, transform=axes[row, col].transAxes, \n                                      verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.8))\n            \n            # Hide empty subplots\n            for idx in range(len(sample_features), n_rows * n_cols):\n                row = idx // n_cols\n                col = idx % n_cols\n                axes[row, col].set_visible(False)\n            \n            plt.suptitle(f'{category} Features Distribution (Sample of {len(sample_features)})', fontsize=16)\n            plt.tight_layout()\n            plt.show()\n\nanalyze_feature_distributions(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:29:45.666866Z","iopub.execute_input":"2025-09-17T17:29:45.667177Z","iopub.status.idle":"2025-09-17T17:30:01.695283Z","shell.execute_reply.started":"2025-09-17T17:29:45.667152Z","shell.execute_reply":"2025-09-17T17:30:01.694205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. CORRELATION ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_correlations(df, feature_categories):\n    \"\"\"Comprehensive correlation analysis\"\"\"\n    \n    print(\"\\n🔗 CORRELATION ANALYSIS\")\n    print(\"=\"*50)\n    \n    # Overall correlation with target\n    if 'forward_returns' in df.columns:\n        target_corr = df.corr()['forward_returns'].sort_values(ascending=False)\n        target_corr = target_corr.drop('forward_returns')  # Remove self-correlation\n        \n        print(\"🎯 TOP 20 FEATURES CORRELATED WITH FORWARD RETURNS:\")\n        print(target_corr.head(20))\n        \n        print(\"\\n🎯 BOTTOM 20 FEATURES CORRELATED WITH FORWARD RETURNS:\")\n        print(target_corr.tail(20))\n        \n        # Visualize top correlations\n        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n        \n        # Top positive correlations\n        top_pos = target_corr.head(15)\n        top_pos.plot(kind='barh', ax=axes[0], color='green', alpha=0.7)\n        axes[0].set_title('Top 15 Positive Correlations with Forward Returns')\n        axes[0].set_xlabel('Correlation')\n        \n        # Top negative correlations\n        top_neg = target_corr.tail(15)\n        top_neg.plot(kind='barh', ax=axes[1], color='red', alpha=0.7)\n        axes[1].set_title('Top 15 Negative Correlations with Forward Returns')\n        axes[1].set_xlabel('Correlation')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # Category-wise correlation analysis\n    for category, features in feature_categories.items():\n        if category == 'Special' or len(features) < 2:\n            continue\n            \n        print(f\"\\n🔍 {category} Internal Correlations:\")\n        \n        # Calculate correlation matrix for this category\n        category_df = df[features].dropna()\n        if len(category_df) > 0 and len(features) > 1:\n            corr_matrix = category_df.corr()\n            \n            # Find highly correlated pairs (> 0.8 or < -0.8)\n            high_corr_pairs = []\n            for i in range(len(corr_matrix.columns)):\n                for j in range(i+1, len(corr_matrix.columns)):\n                    corr_val = corr_matrix.iloc[i, j]\n                    if abs(corr_val) > 0.8:\n                        high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n            \n            if high_corr_pairs:\n                print(f\"High correlations (|r| > 0.8): {len(high_corr_pairs)} pairs\")\n                for pair in high_corr_pairs[:10]:  # Show top 10\n                    print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n            \n            # Visualize correlation heatmap for a subset\n            if len(features) <= 20:\n                plt.figure(figsize=(12, 10))\n                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n                           square=True, linewidths=0.5, fmt='.2f')\n                plt.title(f'{category} Features Correlation Matrix')\n                plt.tight_layout()\n                plt.show()\n\nanalyze_correlations(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:01.696705Z","iopub.execute_input":"2025-09-17T17:30:01.696974Z","iopub.status.idle":"2025-09-17T17:30:08.392474Z","shell.execute_reply.started":"2025-09-17T17:30:01.696954Z","shell.execute_reply":"2025-09-17T17:30:08.389676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🔗 Correlation Analysis  \n\n#### 1. Correlation with Target (`forward_returns`)  \n- As expected, **`market_forward_excess_returns`** is almost perfectly correlated (0.9999+) with `forward_returns`.  \n- Beyond that, only a handful of features have weak positive correlations (e.g., **V13: 0.06**, **M1: 0.04**, **S5: 0.04**).  \n- On the negative side, the strongest correlations are small as well (e.g., **M4: -0.066**, **E7: -0.032**).  \n\n📌 **Implication:**  \nPredictive signal is **extremely weak in linear correlations**, which is typical for financial data. This highlights the need for **non-linear models** and **interaction effects**.  \n\n---\n\n#### 2. Category-Wise Internal Correlations  \n\n- **Macro-Economic (E\\*)**  \n  - Strong redundancy: e.g., **E16–E17 (0.93)**, **E2–E3 (0.90)**.  \n  - Suggests multiple features capture similar macro trends.  \n\n- **Interest Rate (I\\*)**  \n  - Near-perfect correlation **I5–I9 (1.00)** → likely duplicate encodings of the same rate.  \n  - Some overlap across maturities (I4–I8: 0.83).  \n\n- **Price Valuation (P\\*)**  \n  - Very high overlap: **P10–P11 (0.94)**, **P10–P8 (0.81)**.  \n  - Indicates valuation ratios may be derived versions of each other.  \n\n- **Volatility (V\\*)**  \n  - Heavy clustering: e.g., **V10–V9 (0.98)**, **V7–V9 (0.98)**, **V2–V4 (0.94)**.  \n  - Suggests volatility features are highly collinear.  \n\n- **Sentiment (S\\*)**  \n  - Notable correlations: **S10–S11 (0.92)**, **S11–S4 (0.90)**.  \n  - Indicates multiple proxies for investor sentiment track each other closely.  \n\n- **Dummy/Binary (D\\*)**  \n  - **D1–D2 correlation = 1.00** → completely redundant.  \n\n📌 **Implication:**  \n- **Severe multicollinearity** exists within categories.  \n- Feature selection or dimensionality reduction (e.g., **PCA**, **mutual information**, or **domain-pruned sets**) will be crucial to avoid overfitting.  \n\n---\n\n#### 3. Visual Insights  \n- **Bar plots** of correlations confirm that most features hover near zero, reinforcing the idea that simple correlation-based feature ranking is not useful here.  \n- **Heatmaps** (category-level) clearly highlight redundancy clusters (especially in Volatility and Macro).  \n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# 7. TIME SERIES ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_time_series_patterns(df, feature_categories):\n    \"\"\"Analyze time series patterns and trends\"\"\"\n    \n    print(\"\\n📈 TIME SERIES ANALYSIS\")\n    print(\"=\"*50)\n    \n    if 'date_id' not in df.columns:\n        print(\"❌ No date_id column found for time series analysis\")\n        return\n    \n    # Sort by date\n    df_sorted = df.sort_values('date_id')\n    \n    # Analyze target variable over time\n    if 'forward_returns' in df.columns:\n        returns = df_sorted['forward_returns'].dropna()\n        \n        print(\"📊 TIME SERIES STATISTICS:\")\n        print(f\"Total trading days: {len(returns)}\")\n        print(f\"Date range: {df_sorted['date_id'].min()} to {df_sorted['date_id'].max()}\")\n        \n        # Rolling statistics\n        returns_series = pd.Series(returns.values, index=df_sorted.loc[returns.index, 'date_id'])\n        \n        # Calculate rolling statistics\n        window_sizes = [30, 90, 252]  # 1 month, 3 months, 1 year\n        \n        fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n        \n        # Returns over time\n        axes[0, 0].plot(returns_series.index, returns_series.values, alpha=0.7)\n        axes[0, 0].set_title('Forward Returns Over Time')\n        axes[0, 0].set_xlabel('Date ID')\n        axes[0, 0].set_ylabel('Returns')\n        axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n        \n        # Rolling volatility\n        rolling_vol = returns_series.rolling(30).std()\n        axes[0, 1].plot(rolling_vol.index, rolling_vol.values, color='orange')\n        axes[0, 1].set_title('30-Day Rolling Volatility')\n        axes[0, 1].set_xlabel('Date ID')\n        axes[0, 1].set_ylabel('Volatility')\n        \n        # Cumulative returns\n        cumulative_returns = (1 + returns_series).cumprod()\n        axes[1, 0].plot(cumulative_returns.index, cumulative_returns.values, color='green')\n        axes[1, 0].set_title('Cumulative Returns')\n        axes[1, 0].set_xlabel('Date ID')\n        axes[1, 0].set_ylabel('Cumulative Return')\n        \n        # Distribution by year/period\n        # Assuming date_id represents days since a reference point\n        # We'll create bins for different time periods\n        date_bins = pd.cut(df_sorted['date_id'], bins=10)\n        period_returns = df_sorted.groupby(date_bins)['forward_returns'].agg(['mean', 'std', 'count']).dropna()\n        \n        axes[1, 1].bar(range(len(period_returns)), period_returns['mean'], \n                      yerr=period_returns['std'], alpha=0.7, capsize=5)\n        axes[1, 1].set_title('Average Returns by Time Period')\n        axes[1, 1].set_xlabel('Time Period')\n        axes[1, 1].set_ylabel('Average Return')\n        axes[1, 1].tick_params(axis='x', rotation=45)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Seasonality analysis (if we have enough data)\n        if len(returns) > 252 * 2:  # At least 2 years of data\n            print(\"\\n🗓️ SEASONALITY ANALYSIS:\")\n            \n            # Create day of week and month proxies (simplified)\n            # This is a simplified approach - in real scenarios you'd have actual dates\n            df_sorted['day_proxy'] = df_sorted['date_id'] % 7\n            df_sorted['month_proxy'] = (df_sorted['date_id'] // 30) % 12\n            \n            seasonality_stats = df_sorted.groupby('day_proxy')['forward_returns'].agg(['mean', 'std', 'count']).dropna()\n            \n            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n            \n            # Day of week effect\n            axes[0].bar(seasonality_stats.index, seasonality_stats['mean'], alpha=0.7)\n            axes[0].set_title('Average Returns by Day of Week Proxy')\n            axes[0].set_xlabel('Day Proxy')\n            axes[0].set_ylabel('Average Return')\n            \n            # Month effect\n            monthly_stats = df_sorted.groupby('month_proxy')['forward_returns'].agg(['mean', 'std']).dropna()\n            axes[1].bar(monthly_stats.index, monthly_stats['mean'], alpha=0.7)\n            axes[1].set_title('Average Returns by Month Proxy')\n            axes[1].set_xlabel('Month Proxy')\n            axes[1].set_ylabel('Average Return')\n            \n            plt.tight_layout()\n            plt.show()\n\nanalyze_time_series_patterns(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:08.395061Z","iopub.execute_input":"2025-09-17T17:30:08.395573Z","iopub.status.idle":"2025-09-17T17:30:09.900251Z","shell.execute_reply.started":"2025-09-17T17:30:08.395532Z","shell.execute_reply":"2025-09-17T17:30:09.898997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. FEATURE IMPORTANCE AND SELECTION","metadata":{}},{"cell_type":"code","source":"def analyze_feature_importance(df, feature_categories, max_features=50):\n    \"\"\"Analyze feature importance using various methods\"\"\"\n    \n    print(\"\\n⭐ FEATURE IMPORTANCE ANALYSIS\")\n    print(\"=\"*50)\n    \n    if 'forward_returns' not in df.columns:\n        print(\"❌ No target variable found for feature importance analysis\")\n        return\n    \n    # Prepare data - remove non-feature columns\n    feature_cols = []\n    for category, features in feature_categories.items():\n        if category != 'Special':\n            feature_cols.extend(features)\n    \n    # Limit features for computational efficiency\n    if len(feature_cols) > max_features:\n        feature_cols = feature_cols[:max_features]\n        print(f\"⚠️ Limited to first {max_features} features for computational efficiency\")\n    \n    # Create feature matrix\n    X = df[feature_cols].fillna(0)\n    y = df['forward_returns'].fillna(0)\n    \n    # Remove rows where target is missing\n    mask = ~df['forward_returns'].isna()\n    X = X[mask]\n    y = y[mask]\n    \n    if len(X) == 0:\n        print(\"❌ No valid data for feature importance analysis\")\n        return\n    \n    print(f\"📊 Analyzing {len(feature_cols)} features with {len(X)} samples\")\n    \n    # 1. Correlation-based importance\n    corr_importance = abs(X.corrwith(y)).sort_values(ascending=False)\n    \n    # 2. Mutual Information\n    try:\n        mi_scores = mutual_info_regression(X, y, random_state=42)\n        mi_importance = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=False)\n    except:\n        print(\"⚠️ Mutual information calculation failed\")\n        mi_importance = pd.Series(index=feature_cols)\n    \n    # Combine results\n    importance_df = pd.DataFrame({\n        'Correlation': corr_importance,\n        'Mutual_Info': mi_importance\n    }).fillna(0)\n    \n    # Create composite score\n    importance_df['Composite_Score'] = (\n        importance_df['Correlation'].rank(pct=True) + \n        importance_df['Mutual_Info'].rank(pct=True)\n    ) / 2\n    \n    importance_df = importance_df.sort_values('Composite_Score', ascending=False)\n    \n    print(\"\\n🏆 TOP 20 MOST IMPORTANT FEATURES:\")\n    display(importance_df.head(20))\n    \n    # Visualize feature importance\n    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n    \n    # Correlation importance\n    top_corr = importance_df.head(15)['Correlation']\n    top_corr.plot(kind='barh', ax=axes[0], color='blue', alpha=0.7)\n    axes[0].set_title('Top 15 Features by Correlation')\n    axes[0].set_xlabel('Absolute Correlation')\n    \n    # Mutual information importance\n    top_mi = importance_df.head(15)['Mutual_Info']\n    top_mi.plot(kind='barh', ax=axes[1], color='green', alpha=0.7)\n    axes[1].set_title('Top 15 Features by Mutual Information')\n    axes[1].set_xlabel('Mutual Information Score')\n    \n    # Composite score\n    top_composite = importance_df.head(15)['Composite_Score']\n    top_composite.plot(kind='barh', ax=axes[2], color='red', alpha=0.7)\n    axes[2].set_title('Top 15 Features by Composite Score')\n    axes[2].set_xlabel('Composite Score')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df\n\nimportance_results = analyze_feature_importance(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:09.901946Z","iopub.execute_input":"2025-09-17T17:30:09.902284Z","iopub.status.idle":"2025-09-17T17:30:13.706219Z","shell.execute_reply.started":"2025-09-17T17:30:09.902258Z","shell.execute_reply":"2025-09-17T17:30:13.705152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ⭐ Feature Importance Analysis  \n\nWe evaluated the **predictive signal strength** of features using two complementary methods:  \n\n1. **Correlation with target (`forward_returns`)**  \n   - Measures *linear* dependence.  \n2. **Mutual Information (MI)**  \n   - Captures *non-linear* dependencies between features and the target.  \n\nTo reduce redundancy and speed up computation, we limited the analysis to the **first 50 features**.  \n\n---\n\n#### 🏆 Top 20 Most Important Features (Composite Ranking)\n\n| Feature | Correlation | Mutual Info | Composite Score |\n|---------|-------------|-------------|-----------------|\n| **E19** | 0.0238 | 0.1049 | **0.96** |\n| **M17** | 0.0231 | 0.0476 | **0.80** |\n| **E11** | 0.0291 | 0.0427 | **0.79** |\n| **E18** | 0.0080 | 0.0597 | **0.79** |\n| **E12** | 0.0280 | 0.0409 | **0.77** |\n| **M18** | 0.0097 | 0.0562 | **0.77** |\n| **I2**  | 0.0223 | 0.0436 | **0.74** |\n| **E6**  | 0.0153 | 0.0450 | **0.73** |\n| **M4**  | 0.0645 | 0.0368 | **0.71** |\n| **I9**  | 0.0067 | 0.0560 | **0.70** |\n| **E9**  | 0.0136 | 0.0408 | **0.65** |\n| **E16** | 0.0047 | 0.0590 | **0.65** |\n| **I5**  | 0.0066 | 0.0505 | **0.63** |\n| **M10** | 0.0075 | 0.0449 | **0.62** |\n| **P10** | 0.0130 | 0.0403 | **0.62** |\n| **E17** | 0.0053 | 0.0534 | **0.61** |\n| **M8**  | 0.0203 | 0.0338 | **0.61** |\n| **E15** | 0.0057 | 0.0491 | **0.60** |\n| **M5**  | 0.0076 | 0.0403 | **0.58** |\n| **M12** | 0.0228 | 0.0298 | **0.58** |\n\n---\n\n#### 📌 Insights  \n\n- **Macro-Economic (E\\*) features dominate** → E19, E11, E12, E18, E16 appear among the strongest.  \n- **Momentum (M\\*) signals also show up** → M17, M18, M4, M12 provide some signal despite weak linear correlation.  \n- **Interest Rate (I\\*) variables matter** → I2, I5, I9 highlight sensitivity of returns to the yield curve.  \n- **Price Valuation (P10)** contributes, though not as strongly as macro/momentum.  \n- Importantly, all effect sizes are **small** → consistent with the *low signal-to-noise ratio* of financial prediction.  \n\n📊 **Visualizations**:  \n- **Correlation barplot** → most features are near-zero.  \n- **Mutual Information barplot** → MI helps uncover weak but nonlinear relationships.  \n- **Composite ranking** provides a balanced view, mitigating linear-only bias.  \n\n---\n\n⏩ Next step could be to **expand beyond 50 features**, or use **tree-based models (RF, XGBoost, LightGBM)** for a more realistic importance ranking that accounts for feature interactions.  \n","metadata":{}},{"cell_type":"markdown","source":"# 9. DIMENSIONALITY ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_dimensionality(df, feature_categories, n_components=50):\n    \"\"\"Analyze dimensionality and perform PCA\"\"\"\n    \n    print(\"\\n📐 DIMENSIONALITY ANALYSIS\")\n    print(\"=\"*50)\n    \n    # Prepare feature matrix\n    feature_cols = []\n    for category, features in feature_categories.items():\n        if category != 'Special':\n            feature_cols.extend(features)\n    \n    X = df[feature_cols].fillna(0)\n    \n    print(f\"📊 Original dimensions: {X.shape}\")\n    print(f\"📊 Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Perform PCA\n    n_components = min(n_components, X.shape[1], X.shape[0])\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    print(f\"📊 Reduced dimensions: {X_pca.shape}\")\n    \n    # Analyze explained variance\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_variance = np.cumsum(explained_variance_ratio)\n    \n    print(f\"\\n📈 VARIANCE EXPLAINED:\")\n    print(f\"First component: {explained_variance_ratio[0]:.4f}\")\n    print(f\"First 5 components: {cumulative_variance[4]:.4f}\")\n    print(f\"First 10 components: {cumulative_variance[9]:.4f}\")\n    print(f\"First 20 components: {cumulative_variance[19] if len(cumulative_variance) > 19 else cumulative_variance[-1]:.4f}\")\n    \n    # Visualize explained variance\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Individual explained variance\n    axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n    axes[0].set_title('Explained Variance by Component')\n    axes[0].set_xlabel('Principal Component')\n    axes[0].set_ylabel('Explained Variance Ratio')\n    axes[0].set_xlim(0, min(20, len(explained_variance_ratio)))\n    \n    # Cumulative explained variance\n    axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n    axes[1].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% Variance')\n    axes[1].axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% Variance')\n    axes[1].axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% Variance')\n    axes[1].set_title('Cumulative Explained Variance')\n    axes[1].set_xlabel('Number of Components')\n    axes[1].set_ylabel('Cumulative Explained Variance')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Find number of components needed for different variance thresholds\n    thresholds = [0.8, 0.9, 0.95, 0.99]\n    for threshold in thresholds:\n        n_needed = np.argmax(cumulative_variance >= threshold) + 1\n        if cumulative_variance[n_needed-1] >= threshold:\n            print(f\"📊 Components needed for {threshold*100}% variance: {n_needed}\")\n    \n    return pca, X_pca\n\npca_results, pca_data = analyze_dimensionality(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:13.707623Z","iopub.execute_input":"2025-09-17T17:30:13.707981Z","iopub.status.idle":"2025-09-17T17:30:14.581515Z","shell.execute_reply.started":"2025-09-17T17:30:13.707955Z","shell.execute_reply":"2025-09-17T17:30:14.580506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 📐 Dimensionality Analysis (PCA)\n\nTo understand the structure of the feature space, we performed **Principal Component Analysis (PCA)** on the standardized dataset (94 features, excluding \"Special\").  \n\n---\n\n#### 📊 Dataset Dimensionality\n- **Original shape:** (8,990 samples, 94 features)  \n- **Memory usage:** ~6.45 MB  \n- **Reduced shape (after PCA):** (8,990 samples, 50 components)  \n\n---\n\n#### 📈 Variance Explained\n\n- **First component:** 12.6%  \n- **First 5 components:** 41.4%  \n- **First 10 components:** 57.9%  \n- **First 20 components:** 75.0%  \n\n**Components required to reach variance thresholds:**  \n- **80% variance:** 25 components  \n- **90% variance:** 38 components  \n- **95% variance:** 49 components  \n\n---\n\n#### 📌 Insights\n- The **first principal component explains ~12.5% variance**, which is relatively high for financial datasets (suggesting a strong common factor).  \n- By **20 components, 75% of total variance** is explained — showing significant redundancy in the original 94 features.  \n- To retain **95% variance, nearly 49 components are required**, highlighting that while compression is possible, the dataset is still **high-dimensional with dispersed signal**.  \n- These results support the idea that **feature selection / dimensionality reduction** could improve modeling stability and reduce overfitting.  \n\n📊 **Visualizations generated:**  \n1. **Explained variance by component** → steep drop after first ~5–10 components.  \n2. **Cumulative variance plot** → crossing 80%, 90%, and 95% thresholds at 25, 38, and 49 components respectively.  \n","metadata":{}},{"cell_type":"markdown","source":"# 10. OUTLIER DETECTION AND ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_outliers(df, feature_categories, method='iqr'):\n    \"\"\"Comprehensive outlier analysis\"\"\"\n    \n    print(\"\\n🚨 OUTLIER ANALYSIS\")\n    print(\"=\"*50)\n    \n    outlier_summary = {}\n    \n    # Analyze target variable outliers\n    if 'forward_returns' in df.columns:\n        target_data = df['forward_returns'].dropna()\n        \n        if method == 'iqr':\n            Q1 = target_data.quantile(0.25)\n            Q3 = target_data.quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            target_outliers = target_data[(target_data < lower_bound) | (target_data > upper_bound)]\n        else:  # z-score method\n            z_scores = np.abs(stats.zscore(target_data))\n            target_outliers = target_data[z_scores > 3]\n        \n        print(f\"📊 TARGET VARIABLE OUTLIERS:\")\n        print(f\"Total outliers: {len(target_outliers)} ({len(target_outliers)/len(target_data)*100:.2f}%)\")\n        print(f\"Outlier range: [{target_outliers.min():.6f}, {target_outliers.max():.6f}]\")\n        \n        outlier_summary['forward_returns'] = len(target_outliers)\n        \n        # Visualize target outliers\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        \n        # Box plot\n        axes[0].boxplot(target_data, vert=True)\n        axes[0].set_title('Forward Returns Box Plot')\n        axes[0].set_ylabel('Forward Returns')\n        \n        # Histogram with outliers highlighted\n        axes[1].hist(target_data, bins=50, alpha=0.7, label='Normal')\n        axes[1].hist(target_outliers, bins=20, alpha=0.8, color='red', label='Outliers')\n        axes[1].set_title('Forward Returns Distribution')\n        axes[1].set_xlabel('Forward Returns')\n        axes[1].legend()\n        \n        # Time series with outliers highlighted\n        if 'date_id' in df.columns:\n            target_with_date = df[['date_id', 'forward_returns']].dropna()\n            outlier_dates = target_with_date[target_with_date['forward_returns'].isin(target_outliers)]\n            \n            axes[2].plot(target_with_date['date_id'], target_with_date['forward_returns'], \n                        alpha=0.7, label='Normal', linewidth=0.5)\n            axes[2].scatter(outlier_dates['date_id'], outlier_dates['forward_returns'], \n                          color='red', alpha=0.8, s=20, label='Outliers')\n            axes[2].set_title('Forward Returns Over Time with Outliers')\n            axes[2].set_xlabel('Date ID')\n            axes[2].legend()\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # Analyze feature outliers by category\n    for category, features in feature_categories.items():\n        if category == 'Special' or len(features) == 0:\n            continue\n            \n        category_outliers = 0\n        sample_features = features[:min(5, len(features))]  # Sample for efficiency\n        \n        for feature in sample_features:\n            feature_data = df[feature].dropna()\n            if len(feature_data) > 0:\n                if method == 'iqr':\n                    Q1 = feature_data.quantile(0.25)\n                    Q3 = feature_data.quantile(0.75)\n                    IQR = Q3 - Q1\n                    if IQR > 0:\n                        lower_bound = Q1 - 1.5 * IQR\n                        upper_bound = Q3 + 1.5 * IQR\n                        outliers = feature_data[(feature_data < lower_bound) | (feature_data > upper_bound)]\n                        category_outliers += len(outliers)\n                \n        outlier_summary[category] = category_outliers\n        print(f\"📊 {category} outliers (sample): {category_outliers}\")\n    \n    return outlier_summary\n\noutlier_results = analyze_outliers(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:14.582901Z","iopub.execute_input":"2025-09-17T17:30:14.583274Z","iopub.status.idle":"2025-09-17T17:30:15.50672Z","shell.execute_reply.started":"2025-09-17T17:30:14.583246Z","shell.execute_reply":"2025-09-17T17:30:15.505831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🚨 Outlier Analysis\n\nOutliers can significantly impact financial modeling since they often correspond to **rare market shocks** or **structural breaks**.  \nWe conducted outlier detection using the **IQR (Interquartile Range)** method across the target (`forward_returns`) and sampled features from each category.\n\n---\n\n#### 🎯 Target Variable (Forward Returns)\n- **Total outliers:** 598 (~6.65% of samples)  \n- **Outlier range:** [-0.0398, 0.0407]  \n\n📊 **Visualizations Generated:**  \n1. **Box Plot** → highlights extreme values outside IQR whiskers.  \n2. **Histogram** → red overlay showing outliers vs. normal distribution.  \n3. **Time Series Plot** → forward returns over time with outliers flagged in red (revealing whether outliers cluster in specific periods).  \n\n---\n\n#### 📊 Category-Level Outlier Counts (sampled features)\n- **Market Dynamics:** 235 outliers  \n- **Macro Economic:** 3,342 outliers (heavily prone to extreme values — possibly cyclical shocks or regime changes)  \n- **Interest Rate:** 125 outliers  \n- **Price Valuation:** 1,030 outliers  \n- **Volatility:** 519 outliers  \n- **Sentiment:** 277 outliers  \n- **Dummy/Binary:** 0 outliers (as expected due to binary nature)  \n\n---\n\n#### 📌 Insights\n- The **target variable exhibits ~6.65% outliers**, which is typical in financial return data — extreme price moves or corrections.  \n- **Macro-economic features dominate outlier counts**, suggesting they are more volatile or measured across heterogeneous regimes.  \n- **Volatility & Price Valuation features** also produce notable outliers, aligning with their role in capturing market stress.  \n- Dummy/Binary variables are inherently resistant to outliers.  \n\n👉 These results suggest careful **robust scaling** (e.g., Winsorization, robust Z-scores, or quantile transforms) may be needed before modeling.  \n","metadata":{}},{"cell_type":"markdown","source":"# 11. ADVANCED STATISTICAL ANALYSIS","metadata":{}},{"cell_type":"code","source":"def advanced_statistical_analysis(df):\n    \"\"\"Perform advanced statistical tests and analysis\"\"\"\n    \n    print(\"\\n🔬 ADVANCED STATISTICAL ANALYSIS\")\n    print(\"=\"*50)\n    \n    if 'forward_returns' not in df.columns:\n        print(\"❌ No target variable for statistical analysis\")\n        return\n    \n    returns = df['forward_returns'].dropna()\n    \n    # 1. Normality Tests\n    print(\"📊 NORMALITY TESTS:\")\n    jb_stat, jb_pvalue = jarque_bera(returns)\n    print(f\"Jarque-Bera test: statistic={jb_stat:.4f}, p-value={jb_pvalue:.6f}\")\n    \n    if len(returns) <= 5000:  # Shapiro-Wilk for smaller samples\n        sw_stat, sw_pvalue = shapiro(returns)\n        print(f\"Shapiro-Wilk test: statistic={sw_stat:.4f}, p-value={sw_pvalue:.6f}\")\n    \n    # 2. Stationarity Analysis (Augmented Dickey-Fuller Test)\n    try:\n        from statsmodels.tsa.stattools import adfuller\n        adf_result = adfuller(returns)\n        print(f\"\\n📈 STATIONARITY TEST (ADF):\")\n        print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n        print(f\"p-value: {adf_result[1]:.6f}\")\n        print(f\"Critical Values: {adf_result[4]}\")\n        print(f\"Series is {'stationary' if adf_result[1] < 0.05 else 'non-stationary'}\")\n    except ImportError:\n        print(\"⚠️ Statsmodels not available for stationarity tests\")\n    \n    # 3. Autocorrelation Analysis\n    if 'date_id' in df.columns:\n        returns_series = df.set_index('date_id')['forward_returns'].dropna()\n        \n        # Calculate autocorrelations for different lags\n        max_lags = min(50, len(returns_series) // 4)\n        autocorrs = [returns_series.autocorr(lag=i) for i in range(1, max_lags + 1)]\n        \n        print(f\"\\n🔄 AUTOCORRELATION ANALYSIS:\")\n        print(f\"Lag 1 autocorrelation: {autocorrs[0]:.6f}\")\n        print(f\"Lag 5 autocorrelation: {autocorrs[4] if len(autocorrs) > 4 else 'N/A'}\")\n        print(f\"Lag 10 autocorrelation: {autocorrs[9] if len(autocorrs) > 9 else 'N/A'}\")\n        \n        # Plot autocorrelation function\n        plt.figure(figsize=(12, 6))\n        plt.subplot(1, 2, 1)\n        plt.bar(range(1, len(autocorrs) + 1), autocorrs)\n        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n        plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.7)\n        plt.axhline(y=-0.05, color='red', linestyle='--', alpha=0.7)\n        plt.title('Autocorrelation Function')\n        plt.xlabel('Lag')\n        plt.ylabel('Autocorrelation')\n        \n        # Partial autocorrelation (simplified)\n        plt.subplot(1, 2, 2)\n        returns_diff = returns_series.diff().dropna()\n        partial_autocorrs = [returns_diff.autocorr(lag=i) for i in range(1, min(21, len(returns_diff) // 4))]\n        plt.bar(range(1, len(partial_autocorrs) + 1), partial_autocorrs)\n        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n        plt.title('Partial Autocorrelation (Differenced)')\n        plt.xlabel('Lag')\n        plt.ylabel('Partial Autocorrelation')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # 4. Risk Metrics\n    print(f\"\\n⚠️ RISK METRICS:\")\n    print(f\"Value at Risk (5%): {np.percentile(returns, 5):.6f}\")\n    print(f\"Value at Risk (1%): {np.percentile(returns, 1):.6f}\")\n    print(f\"Expected Shortfall (5%): {returns[returns <= np.percentile(returns, 5)].mean():.6f}\")\n    \n    # Maximum drawdown calculation\n    if 'date_id' in df.columns:\n        cumulative = (1 + returns_series).cumprod()\n        running_max = cumulative.expanding().max()\n        drawdown = (cumulative - running_max) / running_max\n        max_drawdown = drawdown.min()\n        print(f\"Maximum Drawdown: {max_drawdown:.6f}\")\n    \n    # 5. Higher Moments Analysis\n    print(f\"\\n📊 HIGHER MOMENTS:\")\n    print(f\"Skewness: {returns.skew():.6f}\")\n    print(f\"Kurtosis: {returns.kurtosis():.6f}\")\n    print(f\"Excess Kurtosis: {returns.kurtosis() - 3:.6f}\")\n    \n    return {\n        'normality_jb_pvalue': jb_pvalue,\n        'autocorr_lag1': autocorrs[0] if 'autocorrs' in locals() else None,\n        'skewness': returns.skew(),\n        'kurtosis': returns.kurtosis(),\n        'var_5pct': np.percentile(returns, 5),\n        'max_drawdown': max_drawdown if 'max_drawdown' in locals() else None\n    }\n\nstatistical_results = advanced_statistical_analysis(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:15.507862Z","iopub.execute_input":"2025-09-17T17:30:15.508158Z","iopub.status.idle":"2025-09-17T17:30:17.039688Z","shell.execute_reply.started":"2025-09-17T17:30:15.508136Z","shell.execute_reply":"2025-09-17T17:30:17.038697Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 📊 Normality Tests\n- **Jarque-Bera test**: p-value ≈ 0.000 → The distribution significantly deviates from normality.  \n- The return distribution shows **non-normal behavior**, which is common in financial time series.\n\n---\n\n### 📈 Stationarity (ADF Test)\n- **ADF Statistic**: -17.51  \n- **p-value**: 0.000  \n- The test strongly rejects the null hypothesis of a unit root.  \n➡️ Conclusion: `forward_returns` series is **stationary**, meaning its statistical properties (mean, variance) remain constant over time.\n\n---\n\n### 🔄 Autocorrelation\n- **Lag 1 autocorrelation**: -0.044 → Mild negative autocorrelation, suggesting slight mean reversion.  \n- **Lag 5 autocorrelation**: -0.024 → Negligible dependence at medium lags.  \n- **Lag 10 autocorrelation**: ≈ 0 → No long-range correlation.  \n\nThis suggests returns are **largely uncorrelated across time**, aligning with the Efficient Market Hypothesis (EMH).\n\n---\n\n### ⚠️ Risk Metrics\n- **Value at Risk (VaR 5%)**: -1.77%  \n- **VaR (1%)**: -3.16%  \n- **Expected Shortfall (5%)**: -2.54% → On average, extreme losses (5% worst cases) are around -2.5%.  \n- **Maximum Drawdown**: -49.24% → Significant downside risk exists, with almost half of value wiped during worst periods.\n\n---\n\n### 📊 Higher Moments\n- **Skewness**: -0.176 → Slight negative skew, meaning left-tail events are marginally heavier.  \n- **Kurtosis**: 2.19 → Less than 3, suggesting a **flatter distribution** than Gaussian.  \n- **Excess Kurtosis**: -0.807 → Thin tails compared to normal distribution (counterintuitive for financial data, which often exhibit fat tails).\n\n---\n\n### ✅ Key Takeaways\n1. Returns are **stationary** but strongly **non-normal**.  \n2. Mild mean reversion at very short lags, but overall returns are uncorrelated.  \n3. Risk profile indicates **high drawdown potential** despite thin-tailed distribution.  \n4. Models assuming Gaussian returns may **underestimate drawdown risk** — robust, non-parametric methods are advisable.\n","metadata":{}},{"cell_type":"markdown","source":"# 12. FEATURE STABILITY ANALYSIS","metadata":{}},{"cell_type":"code","source":"def analyze_feature_stability(df, feature_categories, n_periods=10):\n    \"\"\"Analyze feature stability over time\"\"\"\n    \n    print(\"\\n⚖️ FEATURE STABILITY ANALYSIS\")\n    print(\"=\"*50)\n    \n    if 'date_id' not in df.columns:\n        print(\"❌ No date_id column for stability analysis\")\n        return\n    \n    # Divide data into time periods\n    df_sorted = df.sort_values('date_id')\n    period_size = len(df_sorted) // n_periods\n    \n    stability_results = {}\n    \n    # Sample features from each category for analysis\n    for category, features in feature_categories.items():\n        if category == 'Special' or len(features) == 0:\n            continue\n            \n        sample_features = features[:min(5, len(features))]  # Sample for efficiency\n        category_stability = {}\n        \n        for feature in sample_features:\n            feature_stats = []\n            \n            for i in range(n_periods):\n                start_idx = i * period_size\n                end_idx = (i + 1) * period_size if i < n_periods - 1 else len(df_sorted)\n                \n                period_data = df_sorted.iloc[start_idx:end_idx][feature].dropna()\n                if len(period_data) > 0:\n                    feature_stats.append({\n                        'mean': period_data.mean(),\n                        'std': period_data.std(),\n                        'min': period_data.min(),\n                        'max': period_data.max()\n                    })\n            \n            if len(feature_stats) > 1:\n                # Calculate coefficient of variation for mean and std across periods\n                means = [stat['mean'] for stat in feature_stats]\n                stds = [stat['std'] for stat in feature_stats]\n                \n                mean_stability = np.std(means) / np.mean(np.abs(means)) if np.mean(np.abs(means)) != 0 else np.inf\n                std_stability = np.std(stds) / np.mean(stds) if np.mean(stds) != 0 else np.inf\n                \n                category_stability[feature] = {\n                    'mean_cv': mean_stability,\n                    'std_cv': std_stability,\n                    'periods': len(feature_stats)\n                }\n        \n        stability_results[category] = category_stability\n        \n        # Display stability results for this category\n        print(f\"\\n🔍 {category} Stability (lower = more stable):\")\n        for feature, stability in category_stability.items():\n            print(f\"  {feature}: Mean CV={stability['mean_cv']:.4f}, Std CV={stability['std_cv']:.4f}\")\n    \n    return stability_results\n\nstability_results = analyze_feature_stability(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:17.040761Z","iopub.execute_input":"2025-09-17T17:30:17.0413Z","iopub.status.idle":"2025-09-17T17:30:17.141311Z","shell.execute_reply.started":"2025-09-17T17:30:17.041275Z","shell.execute_reply":"2025-09-17T17:30:17.140301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ✅ Key Takeaways\n1. Features in **Dummy/Binary** and some **Macro_Economic** (like E1, E10) are stable over time.  \n2. **Volatility, Sentiment, and certain Price/Interest features** show high temporal variability — these may require normalization or rolling statistics for model robustness.  \n3. For predictive modeling, consider weighting **stable features more heavily**, or using temporal validation to handle unstable features.\n","metadata":{}},{"cell_type":"markdown","source":"# Advanced EDA Extensions for Financial Time Series Analysis","metadata":{}},{"cell_type":"code","source":"\n# These functions extend existing analysis with sophisticated financial modeling techniques\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom scipy import stats\nfrom scipy.spatial.distance import pdist, squareform\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 1. REGIME DETECTION AND STRUCTURAL BREAKS\ndef detect_regime_changes(df, window=252, threshold=2.0):\n    \"\"\"\n    Detect structural breaks and market regimes using rolling statistics\n    and Chow test approximations\n    \"\"\"\n    print(\"\\n📊 REGIME DETECTION AND STRUCTURAL BREAKS\")\n    print(\"=\"*60)\n    \n    if 'forward_returns' not in df.columns or 'date_id' not in df.columns:\n        print(\"❌ Required columns missing\")\n        return None\n    \n    df_sorted = df.sort_values('date_id')\n    returns = df_sorted['forward_returns'].dropna()\n    \n    # Rolling volatility and mean\n    rolling_vol = returns.rolling(window=window).std()\n    rolling_mean = returns.rolling(window=window).mean()\n    \n    # Detect volatility regime changes\n    vol_zscore = (rolling_vol - rolling_vol.mean()) / rolling_vol.std()\n    vol_breaks = vol_zscore[abs(vol_zscore) > threshold].index\n    \n    # Detect mean reversion regime changes\n    mean_zscore = (rolling_mean - rolling_mean.mean()) / rolling_mean.std()\n    mean_breaks = mean_zscore[abs(mean_zscore) > threshold].index\n    \n    print(f\"Detected {len(vol_breaks)} volatility regime changes\")\n    print(f\"Detected {len(mean_breaks)} mean regime changes\")\n    \n    # Visualize regime changes\n    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n    \n    # Returns with regime markers\n    axes[0].plot(returns.index, returns.values, alpha=0.7, linewidth=0.5)\n    for break_point in vol_breaks:\n        axes[0].axvline(x=break_point, color='red', alpha=0.5, linestyle='--')\n    axes[0].set_title('Returns with Volatility Regime Changes')\n    axes[0].set_ylabel('Returns')\n    \n    # Rolling volatility\n    axes[1].plot(rolling_vol.index, rolling_vol.values, color='orange')\n    axes[1].set_title('Rolling Volatility (252-day)')\n    axes[1].set_ylabel('Volatility')\n    \n    # Rolling mean\n    axes[2].plot(rolling_mean.index, rolling_mean.values, color='green')\n    for break_point in mean_breaks:\n        axes[2].axvline(x=break_point, color='blue', alpha=0.5, linestyle='--')\n    axes[2].set_title('Rolling Mean with Mean Regime Changes')\n    axes[2].set_ylabel('Mean Return')\n    axes[2].set_xlabel('Time')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'vol_breaks': vol_breaks,\n        'mean_breaks': mean_breaks,\n        'rolling_vol': rolling_vol,\n        'rolling_mean': rolling_mean\n    }\n\n# 2. FACTOR EXPOSURE AND LOADING ANALYSIS\ndef analyze_factor_loadings(df, feature_categories, n_factors=5):\n    \"\"\"\n    Perform factor analysis to understand underlying factor structure\n    and exposure to common risk factors\n    \"\"\"\n    print(\"\\n📈 FACTOR LOADING ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Prepare feature matrix\n    feature_cols = []\n    for category, features in feature_categories.items():\n        if category != 'Special':\n            feature_cols.extend(features[:10])  # Limit for computational efficiency\n    \n    X = df[feature_cols].fillna(df[feature_cols].median())\n    \n    if 'forward_returns' in df.columns:\n        y = df['forward_returns'].fillna(0)\n    else:\n        print(\"❌ No target variable found\")\n        return None\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Factor Analysis using PCA as proxy\n    from sklearn.decomposition import PCA, FactorAnalysis\n    \n    try:\n        # Factor Analysis\n        fa = FactorAnalysis(n_components=n_factors, random_state=42)\n        factors = fa.fit_transform(X_scaled)\n        \n        # Calculate factor loadings\n        loadings = fa.components_.T\n        \n        # Create loadings DataFrame\n        loadings_df = pd.DataFrame(\n            loadings, \n            index=feature_cols[:loadings.shape[0]], \n            columns=[f'Factor_{i+1}' for i in range(n_factors)]\n        )\n        \n        print(f\"Top factor loadings for each factor:\")\n        for i in range(n_factors):\n            factor_name = f'Factor_{i+1}'\n            top_loadings = loadings_df[factor_name].abs().nlargest(5)\n            print(f\"\\n{factor_name}:\")\n            for feature, loading in top_loadings.items():\n                print(f\"  {feature}: {loadings_df.loc[feature, factor_name]:.4f}\")\n        \n        # Visualize factor loadings heatmap\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(loadings_df.T, cmap='RdBu_r', center=0, \n                   xticklabels=True, yticklabels=True)\n        plt.title('Factor Loadings Heatmap')\n        plt.xlabel('Features')\n        plt.ylabel('Factors')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n        \n        # Analyze factor exposure to returns\n        factor_returns_corr = np.corrcoef(factors.T, y)[:-1, -1]\n        \n        print(f\"\\nFactor correlations with returns:\")\n        for i, corr in enumerate(factor_returns_corr):\n            print(f\"Factor_{i+1}: {corr:.4f}\")\n        \n        return {\n            'factors': factors,\n            'loadings': loadings_df,\n            'factor_returns_corr': factor_returns_corr\n        }\n        \n    except Exception as e:\n        print(f\"❌ Factor analysis failed: {e}\")\n        return None\n\n# 3. MARKET MICROSTRUCTURE ANALYSIS\ndef analyze_microstructure(df):\n    \"\"\"\n    Analyze market microstructure effects like volatility clustering,\n    leverage effects, and return predictability patterns\n    \"\"\"\n    print(\"\\n🏛️ MARKET MICROSTRUCTURE ANALYSIS\")\n    print(\"=\"*60)\n    \n    if 'forward_returns' not in df.columns:\n        print(\"❌ No returns data for microstructure analysis\")\n        return None\n    \n    df_sorted = df.sort_values('date_id')\n    returns = df_sorted['forward_returns'].dropna()\n    \n    # 1. Volatility Clustering (ARCH effects)\n    returns_squared = returns ** 2\n    \n    # Test for ARCH effects using Ljung-Box test on squared returns\n    from scipy.stats import jarque_bera\n    \n    print(\"🔍 VOLATILITY CLUSTERING ANALYSIS:\")\n    \n    # Calculate autocorrelations of squared returns\n    sq_autocorrs = [returns_squared.autocorr(lag=i) for i in range(1, 21)]\n    significant_lags = sum([1 for ac in sq_autocorrs if abs(ac) > 0.05])\n    print(f\"Significant autocorrelations in squared returns: {significant_lags}/20 lags\")\n    \n    # 2. Leverage Effect Analysis\n    print(\"\\n📉 LEVERAGE EFFECT ANALYSIS:\")\n    \n    # Split returns into positive and negative\n    pos_returns = returns[returns > 0]\n    neg_returns = returns[returns < 0]\n    \n    # Calculate next-day volatility after positive/negative days\n    if len(returns) > 1:\n        next_vol_after_pos = []\n        next_vol_after_neg = []\n        \n        for i in range(len(returns) - 1):\n            if returns.iloc[i] > 0:\n                next_vol_after_pos.append(abs(returns.iloc[i + 1]))\n            elif returns.iloc[i] < 0:\n                next_vol_after_neg.append(abs(returns.iloc[i + 1]))\n        \n        if next_vol_after_pos and next_vol_after_neg:\n            avg_vol_after_pos = np.mean(next_vol_after_pos)\n            avg_vol_after_neg = np.mean(next_vol_after_neg)\n            \n            print(f\"Average volatility after positive days: {avg_vol_after_pos:.6f}\")\n            print(f\"Average volatility after negative days: {avg_vol_after_neg:.6f}\")\n            print(f\"Leverage ratio (neg/pos): {avg_vol_after_neg/avg_vol_after_pos:.4f}\")\n    \n    # 3. Return Predictability Patterns\n    print(\"\\n🔮 RETURN PREDICTABILITY PATTERNS:\")\n    \n    # Momentum vs Mean Reversion\n    momentum_1d = returns.autocorr(lag=1)\n    momentum_5d = returns.autocorr(lag=5)\n    momentum_10d = returns.autocorr(lag=10)\n    \n    print(f\"1-day momentum: {momentum_1d:.6f}\")\n    print(f\"5-day momentum: {momentum_5d:.6f}\")\n    print(f\"10-day momentum: {momentum_10d:.6f}\")\n    \n    # Visualizations\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Autocorrelation of squared returns\n    axes[0, 0].bar(range(1, len(sq_autocorrs) + 1), sq_autocorrs)\n    axes[0, 0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7)\n    axes[0, 0].axhline(y=-0.05, color='red', linestyle='--', alpha=0.7)\n    axes[0, 0].set_title('Autocorrelation of Squared Returns')\n    axes[0, 0].set_xlabel('Lag')\n    axes[0, 0].set_ylabel('Autocorrelation')\n    \n    # Volatility clustering visualization\n    abs_returns = abs(returns)\n    axes[0, 1].plot(abs_returns.index, abs_returns.values, alpha=0.7)\n    axes[0, 1].set_title('Absolute Returns (Volatility Proxy)')\n    axes[0, 1].set_xlabel('Time')\n    axes[0, 1].set_ylabel('Absolute Returns')\n    \n    # Return distribution by sign of previous return\n    if next_vol_after_pos and next_vol_after_neg:\n        axes[1, 0].hist(next_vol_after_pos, alpha=0.7, label='After Positive', bins=30)\n        axes[1, 0].hist(next_vol_after_neg, alpha=0.7, label='After Negative', bins=30)\n        axes[1, 0].set_title('Next-Day Volatility Distribution')\n        axes[1, 0].legend()\n        axes[1, 0].set_xlabel('Volatility')\n        axes[1, 0].set_ylabel('Frequency')\n    \n    # Return autocorrelation\n    autocorrs = [returns.autocorr(lag=i) for i in range(1, 21)]\n    axes[1, 1].bar(range(1, len(autocorrs) + 1), autocorrs)\n    axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n    axes[1, 1].set_title('Return Autocorrelation')\n    axes[1, 1].set_xlabel('Lag')\n    axes[1, 1].set_ylabel('Autocorrelation')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'volatility_clustering': sq_autocorrs,\n        'leverage_effect': avg_vol_after_neg/avg_vol_after_pos if 'avg_vol_after_neg' in locals() else None,\n        'momentum_effects': [momentum_1d, momentum_5d, momentum_10d]\n    }\n\n# 4. FEATURE INTERACTION ANALYSIS\ndef analyze_feature_interactions(df, feature_categories, top_n=10):\n    \"\"\"\n    Analyze non-linear feature interactions and dependencies\n    \"\"\"\n    print(\"\\n🔗 FEATURE INTERACTION ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Select top features from importance analysis\n    feature_cols = []\n    for category, features in feature_categories.items():\n        if category != 'Special':\n            feature_cols.extend(features[:3])  # Top 3 from each category\n    \n    if len(feature_cols) > 20:\n        feature_cols = feature_cols[:20]  # Limit for computational efficiency\n    \n    X = df[feature_cols].fillna(df[feature_cols].median())\n    \n    if 'forward_returns' in df.columns:\n        y = df['forward_returns'].fillna(0)\n    else:\n        print(\"❌ No target variable found\")\n        return None\n    \n    # Calculate interaction effects\n    print(\"🔍 FEATURE INTERACTION EFFECTS:\")\n    \n    # Mutual information between feature pairs and target\n    from sklearn.feature_selection import mutual_info_regression\n    \n    interaction_scores = {}\n    \n    # Create interaction features (products of pairs)\n    for i in range(len(feature_cols)):\n        for j in range(i+1, min(i+5, len(feature_cols))):  # Limit interactions\n            feat1, feat2 = feature_cols[i], feature_cols[j]\n            interaction_feat = X[feat1] * X[feat2]\n            \n            # Calculate mutual information with target\n            mi_score = mutual_info_regression(\n                interaction_feat.values.reshape(-1, 1), \n                y, \n                random_state=42\n            )[0]\n            \n            interaction_scores[f\"{feat1}_x_{feat2}\"] = mi_score\n    \n    # Sort and display top interactions\n    top_interactions = sorted(interaction_scores.items(), \n                            key=lambda x: x[1], reverse=True)[:top_n]\n    \n    print(f\"\\nTop {top_n} feature interactions by mutual information:\")\n    for interaction, score in top_interactions:\n        print(f\"  {interaction}: {score:.6f}\")\n    \n    # Visualize interaction heatmap\n    if len(feature_cols) <= 10:  # Only for manageable number of features\n        interaction_matrix = np.zeros((len(feature_cols), len(feature_cols)))\n        \n        for i in range(len(feature_cols)):\n            for j in range(i+1, len(feature_cols)):\n                feat1, feat2 = feature_cols[i], feature_cols[j]\n                key = f\"{feat1}_x_{feat2}\"\n                if key in interaction_scores:\n                    interaction_matrix[i, j] = interaction_scores[key]\n                    interaction_matrix[j, i] = interaction_scores[key]\n        \n        plt.figure(figsize=(10, 8))\n        sns.heatmap(interaction_matrix, \n                   xticklabels=feature_cols, \n                   yticklabels=feature_cols,\n                   cmap='viridis', annot=True, fmt='.4f')\n        plt.title('Feature Interaction Strength (Mutual Information)')\n        plt.tight_layout()\n        plt.show()\n    \n    return {\n        'top_interactions': top_interactions,\n        'interaction_scores': interaction_scores\n    }\n\n# 5. CLUSTERING AND SEGMENTATION ANALYSIS\ndef perform_market_segmentation(df, feature_categories, n_clusters=5):\n    \"\"\"\n    Perform market regime clustering and segmentation analysis\n    \"\"\"\n    print(\"\\n📊 MARKET SEGMENTATION ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Prepare feature matrix for clustering\n    feature_cols = []\n    for category, features in feature_categories.items():\n        if category != 'Special':\n            feature_cols.extend(features[:5])  # Top 5 from each category\n    \n    if len(feature_cols) > 30:\n        feature_cols = feature_cols[:30]  # Limit features\n    \n    X = df[feature_cols].fillna(df[feature_cols].median())\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Determine optimal number of clusters\n    print(\"🔍 DETERMINING OPTIMAL CLUSTER COUNT:\")\n    \n    silhouette_scores = []\n    k_range = range(2, min(11, len(X)//100))  # Reasonable range\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        cluster_labels = kmeans.fit_predict(X_scaled)\n        silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n        silhouette_scores.append(silhouette_avg)\n        print(f\"  k={k}: Silhouette Score = {silhouette_avg:.4f}\")\n    \n    optimal_k = k_range[np.argmax(silhouette_scores)]\n    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n    \n    # Perform clustering with optimal k\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n    cluster_labels = kmeans.fit_predict(X_scaled)\n    \n    # Add cluster labels to dataframe\n    df_clustered = df.copy()\n    df_clustered['cluster'] = cluster_labels\n    \n    # Analyze cluster characteristics\n    print(f\"\\n📈 CLUSTER CHARACTERISTICS:\")\n    \n    cluster_stats = []\n    for cluster_id in range(optimal_k):\n        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]\n        \n        if 'forward_returns' in cluster_data.columns:\n            returns = cluster_data['forward_returns'].dropna()\n            stats_dict = {\n                'cluster': cluster_id,\n                'size': len(cluster_data),\n                'avg_return': returns.mean(),\n                'volatility': returns.std(),\n                'sharpe': returns.mean() / returns.std() if returns.std() > 0 else 0,\n                'min_return': returns.min(),\n                'max_return': returns.max()\n            }\n            cluster_stats.append(stats_dict)\n            \n            print(f\"\\nCluster {cluster_id} ({len(cluster_data)} samples):\")\n            print(f\"  Average Return: {stats_dict['avg_return']:.6f}\")\n            print(f\"  Volatility: {stats_dict['volatility']:.6f}\")\n            print(f\"  Sharpe Ratio: {stats_dict['sharpe']:.4f}\")\n    \n    cluster_stats_df = pd.DataFrame(cluster_stats)\n    \n    # Visualizations\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Silhouette scores\n    axes[0, 0].plot(k_range, silhouette_scores, 'bo-')\n    axes[0, 0].set_title('Silhouette Score by Number of Clusters')\n    axes[0, 0].set_xlabel('Number of Clusters')\n    axes[0, 0].set_ylabel('Silhouette Score')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Cluster returns distribution\n    if 'forward_returns' in df_clustered.columns:\n        for cluster_id in range(optimal_k):\n            cluster_returns = df_clustered[df_clustered['cluster'] == cluster_id]['forward_returns'].dropna()\n            axes[0, 1].hist(cluster_returns, alpha=0.6, label=f'Cluster {cluster_id}', bins=30)\n        axes[0, 1].set_title('Return Distribution by Cluster')\n        axes[0, 1].set_xlabel('Returns')\n        axes[0, 1].legend()\n    \n    # Cluster characteristics\n    if cluster_stats:\n        cluster_stats_df.set_index('cluster')[['avg_return', 'volatility']].plot(kind='bar', ax=axes[1, 0])\n        axes[1, 0].set_title('Average Return and Volatility by Cluster')\n        axes[1, 0].set_ylabel('Value')\n        axes[1, 0].tick_params(axis='x', rotation=0)\n    \n    # t-SNE visualization (if computationally feasible)\n    if len(X_scaled) <= 5000:  # Limit for t-SNE\n        try:\n            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_scaled)//4))\n            X_tsne = tsne.fit_transform(X_scaled)\n            \n            scatter = axes[1, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n            axes[1, 1].set_title('t-SNE Visualization of Clusters')\n            axes[1, 1].set_xlabel('t-SNE 1')\n            axes[1, 1].set_ylabel('t-SNE 2')\n            plt.colorbar(scatter, ax=axes[1, 1])\n        except:\n            axes[1, 1].text(0.5, 0.5, 't-SNE visualization\\nnot available', \n                           ha='center', va='center', transform=axes[1, 1].transAxes)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'cluster_labels': cluster_labels,\n        'cluster_stats': cluster_stats_df,\n        'optimal_k': optimal_k,\n        'silhouette_scores': silhouette_scores\n    }\n\n# 6. ADVANCED FEATURE ENGINEERING INSIGHTS\ndef advanced_feature_engineering_analysis(df, feature_categories):\n    \"\"\"\n    Provide insights for advanced feature engineering\n    \"\"\"\n    print(\"\\n⚙️ ADVANCED FEATURE ENGINEERING INSIGHTS\")\n    print(\"=\"*60)\n    \n    insights = {}\n    \n    # 1. Non-linearity Detection\n    print(\"🔍 NON-LINEARITY DETECTION:\")\n    \n    if 'forward_returns' in df.columns:\n        target = df['forward_returns'].fillna(0)\n        \n        # Sample features for analysis\n        sample_features = []\n        for category, features in feature_categories.items():\n            if category != 'Special':\n                sample_features.extend(features[:3])\n        \n        if len(sample_features) > 15:\n            sample_features = sample_features[:15]\n        \n        nonlinear_candidates = []\n        \n        for feature in sample_features:\n            if feature in df.columns:\n                feat_data = df[feature].fillna(df[feature].median())\n                \n                # Test polynomial relationship\n                try:\n                    linear_corr = abs(np.corrcoef(feat_data, target)[0, 1])\n                    \n                    # Quadratic transformation\n                    quad_feat = feat_data ** 2\n                    quad_corr = abs(np.corrcoef(quad_feat, target)[0, 1])\n                    \n                    # Log transformation (if possible)\n                    if (feat_data > 0).all():\n                        log_feat = np.log(feat_data)\n                        log_corr = abs(np.corrcoef(log_feat, target)[0, 1])\n                    else:\n                        log_corr = 0\n                    \n                    max_nonlinear_corr = max(quad_corr, log_corr)\n                    \n                    if max_nonlinear_corr > linear_corr * 1.2:  # 20% improvement\n                        nonlinear_candidates.append({\n                            'feature': feature,\n                            'linear_corr': linear_corr,\n                            'best_nonlinear_corr': max_nonlinear_corr,\n                            'improvement': (max_nonlinear_corr - linear_corr) / linear_corr\n                        })\n                \n                except:\n                    continue\n        \n        # Sort by improvement\n        nonlinear_candidates.sort(key=lambda x: x['improvement'], reverse=True)\n        \n        print(f\"Found {len(nonlinear_candidates)} features with significant non-linear relationships:\")\n        for candidate in nonlinear_candidates[:5]:\n            print(f\"  {candidate['feature']}: {candidate['improvement']:.2%} improvement\")\n        \n        insights['nonlinear_candidates'] = nonlinear_candidates\n    \n    # 2. Temporal Feature Opportunities\n    print(\"\\n⏰ TEMPORAL FEATURE OPPORTUNITIES:\")\n    \n    temporal_insights = []\n    \n    # Day of week effects\n    if 'date_id' in df.columns and 'forward_returns' in df.columns:\n        df_temp = df.copy()\n        df_temp['day_of_week'] = df_temp['date_id'] % 7\n        \n        dow_returns = df_temp.groupby('day_of_week')['forward_returns'].mean()\n        dow_std = dow_returns.std()\n        \n        if dow_std > 0.001:  # Significant variation\n            temporal_insights.append(\"Day-of-week effects detected - consider categorical encoding\")\n    \n    # Volatility regime features\n    if 'forward_returns' in df.columns:\n        returns = df['forward_returns'].fillna(0)\n        vol_regime = (returns.rolling(20).std() > returns.rolling(20).std().median()).astype(int)\n        regime_corr = abs(np.corrcoef(vol_regime, returns)[0, 1])\n        \n        if regime_corr > 0.01:\n            temporal_insights.append(\"Volatility regime features could be valuable\")\n    \n    print(f\"Temporal feature recommendations:\")\n    for insight in temporal_insights:\n        print(f\"  • {insight}\")\n    \n    insights['temporal_insights'] = temporal_insights\n    \n    # 3. Feature Combination Recommendations\n    print(\"\\n🔧 FEATURE COMBINATION RECOMMENDATIONS:\")\n    \n    combination_recs = [\n        \"Consider ratio features between P* (valuation) and E* (macro) variables\",\n        \"Volatility-adjusted momentum: M* features divided by V* features\", \n        \"Sentiment-weighted indicators: S* features multiplied by market stress indicators\",\n        \"Interest rate slope features: differences between I* features of different maturities\",\n        \"Relative strength indicators: rank-based transformations of features within categories\"\n    ]\n    \n    for i, rec in enumerate(combination_recs, 1):\n        print(f\"  {i}. {rec}\")\n    \n    insights['combination_recommendations'] = combination_recs\n    \n    return insights\n\n# Usage example:\ndef run_advanced_eda_suite(df, feature_categories):\n    \"\"\"\n    Run the complete advanced EDA suite\n    \"\"\"\n    print(\"🚀 RUNNING ADVANCED EDA SUITE\")\n    print(\"=\"*80)\n    \n    results = {}\n    \n    try:\n        results['regime_analysis'] = detect_regime_changes(df)\n    except Exception as e:\n        print(f\"⚠️ Regime analysis failed: {e}\")\n    \n    try:\n        results['factor_analysis'] = analyze_factor_loadings(df, feature_categories)\n    except Exception as e:\n        print(f\"⚠️ Factor analysis failed: {e}\")\n    \n    try:\n        results['microstructure'] = analyze_microstructure(df)\n    except Exception as e:\n        print(f\"⚠️ Microstructure analysis failed: {e}\")\n    \n    try:\n        results['interactions'] = analyze_feature_interactions(df, feature_categories)\n    except Exception as e:\n        print(f\"⚠️ Interaction analysis failed: {e}\")\n    \n    try:\n        results['segmentation'] = perform_market_segmentation(df, feature_categories)\n    except Exception as e:\n        print(f\"⚠️ Segmentation analysis failed: {e}\")\n    \n    try:\n        results['feature_engineering'] = advanced_feature_engineering_analysis(df, feature_categories)\n    except Exception as e:\n        print(f\"⚠️ Feature engineering analysis failed: {e}\")\n    \n    print(\"\\n✅ ADVANCED EDA SUITE COMPLETED\")\n    print(\"=\"*80)\n    \n    return results\n\n# Example usage:\nadvanced_results = run_advanced_eda_suite(train_df, feature_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:30:17.144131Z","iopub.execute_input":"2025-09-17T17:30:17.144417Z","iopub.status.idle":"2025-09-17T17:30:43.694905Z","shell.execute_reply.started":"2025-09-17T17:30:17.144394Z","shell.execute_reply":"2025-09-17T17:30:43.693785Z"}},"outputs":[],"execution_count":null}]}