# EXP-005: Fundamental Strategy Pivot

## í˜„ì‹¤ ì§ì‹œ: k ì¡°ì •ìœ¼ë¡œëŠ” 17ì  ë¶ˆê°€ëŠ¥

### ì¦ê±°
- **ìš°ë¦¬ ìµœê³ **: CV Sharpe 0.836 â†’ Kaggle 0.15~0.44
- **ìƒìœ„ê¶Œ**: 17.333 (ìš°ë¦¬ì˜ **38~115ë°°**)
- **k=500 ì‹¤íŒ¨**: CVì—ì„œ ìµœê³ ì˜€ì§€ë§Œ ì‹¤ì œë¡œëŠ” ìµœì•…

### ê·¼ë³¸ ì›ì¸
1. **ëª¨ë¸ ì˜ˆì¸¡ë ¥ ë¶€ì¡±**: Lassoì˜ excess return ì˜ˆì¸¡ ìƒê´€ê³„ìˆ˜ 0.03~0.06 ìˆ˜ì¤€
2. **këŠ” ì¦í­ê¸°ì¼ ë¿**: ì•½í•œ ì‹ í˜¸ Ã— 500 = í° ë…¸ì´ì¦ˆ
3. **ì„ í˜• ëª¨ë¸ì˜ í•œê³„**: ì‹œìž¥ì€ ë¹„ì„ í˜•, ì‹œë³€ì , ë³µìž¡ê³„

**ê²°ë¡ **: í˜„ìž¬ Lasso + Top-20 + k ì¡°ì • ì ‘ê·¼ì€ í•œê³„ ëª…í™•. **ì „ëžµ ì „í™˜ í•„ìš”.**

---

## ðŸŽ¯ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©í–¥

### Option 1: ì‹œê³„ì—´ ëª¨ë¸ (Regime-Aware) â­â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì‹œìž¥ì€ regimeì´ ë°”ë€œ (bull/bear/sideways). ë‹¨ì¼ ëª¨ë¸ì´ ì•„ë‹Œ regimeë³„ ëª¨ë¸

**êµ¬í˜„**:
```python
# 1. Regime íƒì§€ (HMM, Clustering, Rule-based)
regime = detect_regime(features)  # 0=bear, 1=neutral, 2=bull

# 2. Regimeë³„ ëª¨ë¸
if regime == 0:  # Bear market
    position = 0.0~0.8  # ë°©ì–´ì 
elif regime == 1:  # Neutral
    position = 0.8~1.2  # ì¤‘ë¦½
else:  # Bull
    position = 1.2~2.0  # ê³µê²©ì 

# 3. Ensemble
models = {
    'bear': train_model(data[bear_periods]),
    'neutral': train_model(data[neutral_periods]),
    'bull': train_model(data[bull_periods])
}
```

**Feature candidates for regime detection**:
- V13, V10, V9 (volatility features) - ìƒìœ„ ìƒê´€
- M4, M1, M2 (macro features)
- D1~D9 (date/calendar features) - seasonality

**ìž¥ì **:
- ë¶„í¬ ì´ë™ ë¬¸ì œ í•´ê²° (regimeë³„ ëª¨ë¸)
- ë¹„ì„ í˜•ì„± í¬ì°©
- ìƒìœ„ê¶Œì´ ì‚¬ìš©í•  ê°€ëŠ¥ì„± ë†’ìŒ

**ë‹¨ì **:
- ë³µìž¡ë„ ì¦ê°€
- Regime ì˜¤ë¶„ë¥˜ ë¦¬ìŠ¤í¬

### Option 2: Gradient Boosting (XGBoost/LightGBM) â­â­â­â­

**ì™œ Lassoê°€ ì•„ë‹Œê°€**:
- Lasso: ì„ í˜•, feature interaction í¬ì°© ëª»í•¨
- XGBoost: ë¹„ì„ í˜•, feature interaction ìžë™ í•™ìŠµ

**êµ¬í˜„**:
```python
import xgboost as xgb

# ì „ì²´ 98ê°œ feature ì‚¬ìš© (LassoëŠ” Top-20ë§Œ)
model = xgb.XGBRegressor(
    n_estimators=500,
    learning_rate=0.01,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='reg:squarederror'
)

# TimeSeriesSplit CV
# Target: market_forward_excess_returns (not position)
model.fit(X_train, y_train)

# Feature importance ë¶„ì„
# Top featuresê°€ Top-20ê³¼ ë‹¤ë¥¼ ê°€ëŠ¥ì„± (interaction ê³ ë ¤)
```

**ìž¥ì **:
- Feature interaction ìžë™ í•™ìŠµ
- ë¹„ì„ í˜• ê´€ê³„ í¬ì°©
- Kaggle competitionì—ì„œ ê²€ì¦ëœ ê°•ë ¥í•¨

**ë‹¨ì **:
- Overfitting ë¦¬ìŠ¤í¬ (TimeSeriesSplitìœ¼ë¡œ ë°©ì–´)
- í•´ì„ì„± ë‚®ìŒ

### Option 3: Ensemble (Multiple Models + Multiple k) â­â­â­

**ì•„ì´ë””ì–´**: ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ ê²°í•©

```python
models = {
    'lasso_top20': Lasso(alpha=1e-4) on top-20,
    'lasso_top50': Lasso(alpha=1e-4) on top-50,
    'ridge': Ridge(alpha=1.0) on all features,
    'xgb': XGBRegressor(...),
    'lgbm': LGBMRegressor(...)
}

# ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”
excess_pred = weighted_average([
    models['lasso_top20'].predict(X) * 0.2,
    models['xgb'].predict(X) * 0.4,
    models['lgbm'].predict(X) * 0.4
])

# k ê°’ë„ ë™ì  ì¡°ì • (confidence-weighted)
k_effective = base_k * confidence_score(excess_pred)
```

**ìž¥ì **:
- ëª¨ë¸ ë‹¤ì–‘ì„±ìœ¼ë¡œ robustness ì¦ê°€
- ë‹¨ì¼ ëª¨ë¸ ì‹¤íŒ¨ ë¦¬ìŠ¤í¬ ë¶„ì‚°

**ë‹¨ì **:
- ë³µìž¡ë„ ë§¤ìš° ë†’ìŒ
- ê³¼ì í•© ìœ„í—˜

### Option 4: Feature Engineering ëŒ€ê³µì‚¬ â­â­â­â­

**í˜„ìž¬ ë¬¸ì œ**: Top-20 featuresëŠ” ë‹¨ìˆœ correlation ê¸°ë°˜
- M4, V13, M1 ë“± ê°œë³„ featureë§Œ ì‚¬ìš©
- Feature interaction ë¯¸ê³ ë ¤

**ìƒˆ Feature ìƒì„±**:
```python
# 1. Interaction features
X['M4_x_V13'] = X['M4'] * X['V13']
X['M1_x_S5'] = X['M1'] * X['S5']

# 2. Lag features (ì‹œê³„ì—´ íŒ¨í„´)
for col in ['M4', 'V13', 'M1']:
    X[f'{col}_lag1'] = X[col].shift(1)
    X[f'{col}_lag5'] = X[col].shift(5)
    X[f'{col}_rolling_mean_10'] = X[col].rolling(10).mean()
    X[f'{col}_rolling_std_10'] = X[col].rolling(10).std()

# 3. Ratio/Diff features
X['M4_M1_ratio'] = X['M4'] / (X['M1'] + 1e-6)
X['V13_diff'] = X['V13'].diff()

# 4. Regime indicators
X['high_vol'] = (X['V13'] > X['V13'].rolling(50).quantile(0.8)).astype(int)
X['low_vol'] = (X['V13'] < X['V13'].rolling(50).quantile(0.2)).astype(int)

# 5. Calendar features (D1~D9 í™œìš©)
# D4=1 (ì›”ìš”ì¼), D5=1 (ê¸ˆìš”ì¼) ë“± â†’ calendar effect
X['is_monday'] = X['D4']
X['is_friday'] = X['D5']
```

**ìž¥ì **:
- Lasso/Ridgeë„ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
- XGBoostì™€ ì¡°í•© ì‹œ ì‹œë„ˆì§€

**ë‹¨ì **:
- Feature ìˆ˜ í­ë°œ (curse of dimensionality)
- Overfitting ìœ„í—˜

### Option 5: Direct Position Classification â­â­

**ì•„ì´ë””ì–´**: Regression ëŒ€ì‹  Classification
- Class 0: position = 0~0.5 (defensive)
- Class 1: position = 0.5~1.0 (neutral-defensive)
- Class 2: position = 1.0~1.5 (neutral-aggressive)
- Class 3: position = 1.5~2.0 (aggressive)

```python
# Target ë³€í™˜
y_class = pd.cut(
    positions_train,  # [0, 2]
    bins=[0, 0.5, 1.0, 1.5, 2.0],
    labels=[0, 1, 2, 3]
)

# Multi-class classifier
clf = xgb.XGBClassifier(...)
clf.fit(X_train, y_class)

# Prediction
class_pred = clf.predict(X_test)
position = class_to_position_mapping[class_pred]
```

**ìž¥ì **:
- ê·¹ë‹¨ í¬ì§€ì…˜(0, 2)ì„ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµ
- Softmax probabilityë¡œ confidence ì¸¡ì • ê°€ëŠ¥

**ë‹¨ì **:
- H1a ì‹¤í—˜ì—ì„œ ì‹¤íŒ¨í–ˆìŒ (Sharpe 0.631~0.671)
- Continuous targetì„ discretizeí•˜ë©´ ì •ë³´ ì†ì‹¤

---

## ðŸ”¬ ì¶”ì²œ ì‹¤í—˜ ê³„íš (EXP-005)

### Phase 1: Quick Wins (1~2ì¼)

**H1: XGBoost Baseline**
- ì „ì²´ 98 features ì‚¬ìš©
- XGBRegressor(n_estimators=500, max_depth=6)
- Target: market_forward_excess_returns
- k=50, 100, 200 í…ŒìŠ¤íŠ¸
- **ì˜ˆìƒ**: Lasso ëŒ€ë¹„ 10~20% ì„±ëŠ¥ í–¥ìƒ

**H2: Feature Engineering + Lasso**
- Lag features (1, 5, 10ì¼)
- Rolling statistics (mean, std, 10ì¼)
- Interaction features (M4Ã—V13, M1Ã—S5 ë“±)
- Lasso Top-50 features
- **ì˜ˆìƒ**: Lasso ëŒ€ë¹„ 5~15% í–¥ìƒ

### Phase 2: Advanced (3~5ì¼)

**H3: Regime-Based Ensemble**
- Volatility regime íƒì§€ (V13 ê¸°ì¤€)
- High/Medium/Low vol ë³„ ëª¨ë¸
- k ê°’ë„ regimeë³„ ì¡°ì •
- **ì˜ˆìƒ**: ë¶„í¬ ì´ë™ ë¬¸ì œ í•´ê²°, 20~40% í–¥ìƒ

**H4: XGBoost + LightGBM Ensemble**
- XGBoost, LightGBM, CatBoost ì•™ìƒë¸”
- Weighted average (Sharpe ê¸°ë°˜ ê°€ì¤‘ì¹˜)
- **ì˜ˆìƒ**: ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ 5~10% í–¥ìƒ

### Phase 3: Moonshot (1ì£¼)

**H5: Deep Learning (LSTM/Transformer)**
- ì‹œê³„ì—´ íŠ¹í™” ëª¨ë¸
- Attention mechanismìœ¼ë¡œ ì¤‘ìš” ì‹œì  í•™ìŠµ
- **ë¦¬ìŠ¤í¬**: ë°ì´í„° ë¶€ì¡±(8990í–‰), Overfitting ìœ„í—˜ ë†’ìŒ
- **ì˜ˆìƒ**: ì„±ê³µ ì‹œ 30~50% í–¥ìƒ, ì‹¤íŒ¨ ê°€ëŠ¥ì„± 50%

---

## ðŸ“Š ë°ì´í„° ë¶„ì„ ìž¬ê²€í† 

### í˜„ìž¬ ì‚¬ìš© ì¤‘ì¸ Top-20 features (EXP-000)
```
M4 (-0.066), V13 (0.062), M1 (0.046), S5 (0.040), S2 (-0.038),
D1 (0.034), D2 (0.034), M2 (0.033), V10 (0.033), E7 (-0.032), ...
```

**ë¬¸ì œ**:
1. **ìƒê´€ê³„ìˆ˜ê°€ ë„ˆë¬´ ì•½í•¨** (max 0.066)
2. **ì„ í˜• ìƒê´€ë§Œ ê³ ë ¤** (ë¹„ì„ í˜• ê´€ê³„ ë†“ì¹¨)
3. **Feature interaction ë¬´ì‹œ**

### ìƒˆë¡œìš´ Feature ë¶„ì„ í•„ìš”

**ê·¸ë£¹ë³„ ì „ëžµ**:
- **V (Volatility)**: V13, V10, V9 â†’ Regime íƒì§€ í•µì‹¬
- **M (Macro)**: M4, M1, M2 â†’ ê²½ì œ í™˜ê²½
- **S (Sentiment)**: S5, S2 â†’ íˆ¬ìž ì‹¬ë¦¬
- **D (Date)**: D1~D9 â†’ Calendar effects (ìš”ì¼, ì›”ë§ ë“±)
- **P (Price)**: ê°€ê²© ëª¨ë©˜í…€ featureë“¤
- **E (Events)**, **I (Index)**: ë³´ì¡° ì‹ í˜¸

**Missing data ì „ëžµ**:
- E7 (77.5%), V10 (67.3%), S3 (63.8%), M1 (61.7%) â†’ ê²°ì¸¡ ì‹¬ê°
- í˜„ìž¬: ë‹¨ìˆœ median imputation
- ê°œì„ : MICE, Forward-fill (ì‹œê³„ì—´), ë˜ëŠ” Missing indicator

---

## ðŸŽ¯ ìµœìš°ì„  ì‹¤í–‰ ê¶Œìž¥

**ì œì•ˆ**: **H1 (XGBoost Baseline) + H2 (Feature Engineering)** ë™ì‹œ ì§„í–‰

**ì´ìœ **:
1. **XGBoostëŠ” Kaggle í‘œì¤€**: ê±°ì˜ ëª¨ë“  ìƒìœ„ê¶Œì´ ì‚¬ìš©
2. **ë¹ ë¥¸ ê²€ì¦**: 1~2ì¼ ë‚´ ê²°ê³¼ í™•ì¸
3. **Feature Engineeringì€ ë²”ìš©**: ëª¨ë“  ëª¨ë¸ì— ë„ì›€
4. **LassoëŠ” ë„ˆë¬´ ì•½í•¨**: ì„ í˜• ëª¨ë¸ë¡œëŠ” í•œê³„

**êµ¬ì²´ì  ì‹¤í–‰**:
```bash
# EXP-005 ìƒì„±
mkdir -p experiments/005
touch experiments/005/HYPOTHESES.md
touch experiments/005/run_xgboost.py
touch experiments/005/feature_engineering.py
touch experiments/005/README.md

# H1: XGBoost baseline
# H2: Lasso + feature engineering
# H3: Regime-based XGBoost
# H4: Ensemble (XGBoost + LightGBM + Lasso)
```

---

## ðŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### Why k adjustment failed
1. **Garbage In, Garbage Out**: Lasso ì˜ˆì¸¡ ìƒê´€ 0.03~0.06 ìˆ˜ì¤€
2. **këŠ” í™•ëŒ€ê²½**: ì•½í•œ ì‹ í˜¸ë¥¼ í‚¤ì›Œë„ ì—¬ì „ížˆ ì•½í•¨
3. **ë¶„í¬ ì´ë™**: í›ˆë ¨(2020~2023) vs í…ŒìŠ¤íŠ¸(2024+)ëŠ” ë‹¤ë¥¸ ì„¸ê³„

### What top performers likely do
1. **Gradient Boosting**: XGBoost/LightGBM/CatBoost ì•™ìƒë¸”
2. **Feature Engineering**: Lag, Rolling, Interaction features
3. **Regime Detection**: Volatility/Market regimeë³„ ì „ëžµ
4. **Sophisticated Ensembling**: 10+ models ê²°í•©
5. **Advanced Imputation**: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ìµœì í™”

### Reality check
- **ìš°ë¦¬**: Lasso + Top-20 â†’ 0.15~0.44
- **ìƒìœ„ê¶Œ**: ì•„ë§ˆë„ XGBoost + Feature Engineering + Regime â†’ 17+
- **Gap**: **ëª¨ë¸ ìžì²´ê°€ ë‹¤ë¦„**, kë§Œ ë°”ê¿”ì„œëŠ” ëª» ë”°ë¼ê°

---

## ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ**:
1. âœ… k=200 ì œì¶œ (ì´ë¯¸ ì¤€ë¹„ë¨) â†’ ì ìˆ˜ í™•ì¸
2. â­ï¸ EXP-005 ì‹œìž‘: XGBoost + Feature Engineering
3. â­ï¸ 1~2ì¼ ë‚´ ìƒˆ ì ‘ê·¼ìœ¼ë¡œ ì œì¶œ

**ì¤‘ê¸°**:
1. Regime-based modeling
2. Ensemble (XGBoost + LightGBM)
3. Advanced feature engineering (lag, rolling, interaction)

**ìž¥ê¸°**:
- Deep Learning ì‹œë„ (LSTM/Transformer)
- Kaggle discussion/notebooks ì°¸ê³  (ìƒìœ„ê¶Œ ê³µìœ  ëŒ€ê¸°)

---

**ìž‘ì„±ì¼**: 2025-10-03
**í˜„ìž¬ ìƒíƒœ**: Lasso + k ì¡°ì • í•œê³„ ëª…í™•, ì „ëžµ ì „í™˜ í•„ìš”
**ëª©í‘œ**: 17+ ì ìˆ˜ ë‹¬ì„± ìœ„í•´ ê·¼ë³¸ì  ì ‘ê·¼ ë³€ê²½
