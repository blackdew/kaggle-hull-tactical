# EXP-005: Gradient Boosting + Feature Engineering

## ê°œìš”

**ëª©í‘œ**: Lassoë¥¼ XGBoost/LightGBMìœ¼ë¡œ êµì²´í•˜ì—¬ ì˜ˆì¸¡ë ¥ í–¥ìƒ

**ë°°ê²½**:
- EXP-004 ì‹¤íŒ¨: Lasso + k ì¡°ì •ìœ¼ë¡œëŠ” í•œê³„ ëª…í™• (Kaggle 0.15~0.44)
- ìƒìœ„ê¶Œ 17.333ì ê³¼ 38~115ë°° ì°¨ì´
- ê·¼ë³¸ ì›ì¸: ëª¨ë¸ ìì²´ì˜ ì˜ˆì¸¡ë ¥ ë¶€ì¡± (ìƒê´€ 0.03~0.06 ìˆ˜ì¤€)

**ì ‘ê·¼**:
- Lasso (ì„ í˜•) â†’ XGBoost/LightGBM (ë¹„ì„ í˜•, feature interaction)
- Top-20 features â†’ ì „ì²´ 98 features + engineered features
- Feature engineering: Lag, Rolling, Interaction

---

## ì‹¤í—˜ ëª©ë¡

| ì‹¤í—˜ | ëª¨ë¸ | Features | k ê°’ | ì˜ˆìƒ Sharpe | ìš°ì„ ìˆœìœ„ |
|------|------|----------|------|------------|---------|
| **H1** | XGBoost | ì „ì²´ 98ê°œ | 50, 100, 200 | 0.7~0.9 | â­â­â­â­â­ |
| **H2** | LightGBM | ì „ì²´ 98ê°œ | 50, 100, 200 | 0.7~0.9 | â­â­â­â­ |
| **H3** | XGBoost | + Lag/Rolling | 50, 100, 200 | 0.75~0.95 | â­â­â­â­â­ |
| **H4** | XGBoost | + Interaction | 50, 100, 200 | 0.8~1.0 | â­â­â­â­ |
| **H5** | Ensemble | XGB+LGBM+Lasso | 50, 100, 200 | 0.8~1.0 | â­â­â­ |
| **H6** | XGBoost | Regime-based | regimeë³„ | 0.85~1.05 | â­â­â­ |

**í•„ìˆ˜**: H1, H2, H3
**ì„ íƒ**: H4, H5, H6 (ì‹œê°„ ìˆìœ¼ë©´)

---

## ë¹ ë¥¸ ì‹¤í–‰

### ì „ì²´ ì‹¤í—˜ (Phase 1~3)
```bash
# H1: XGBoost Baseline
uv run python experiments/005/run_experiments.py --hypothesis H1

# H2: LightGBM Baseline
uv run python experiments/005/run_experiments.py --hypothesis H2

# H3: Feature Engineering
uv run python experiments/005/run_experiments.py --hypothesis H3

# H4~H6: Advanced (ì„ íƒ)
uv run python experiments/005/run_experiments.py --hypothesis H4
uv run python experiments/005/run_experiments.py --hypothesis H5
uv run python experiments/005/run_experiments.py --hypothesis H6

# ì „ì²´ ì‹¤í–‰ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, 4~6ì‹œê°„+)
uv run python experiments/005/run_experiments.py --all
```

### Phaseë³„ ì‹¤í–‰
```bash
# Phase 1: Baseline models (H1, H2)
uv run python experiments/005/run_experiments.py --phase 1

# Phase 2: Feature engineering (H3, H4)
uv run python experiments/005/run_experiments.py --phase 2

# Phase 3: Advanced (H5, H6)
uv run python experiments/005/run_experiments.py --phase 3
```

---

## ìƒì„¸ ì‹¤í–‰ ê°€ì´ë“œ

### 1. í™˜ê²½ í™•ì¸
```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install xgboost lightgbm scikit-learn pandas numpy

# ë°ì´í„° í™•ì¸
ls data/train.csv  # ìˆì–´ì•¼ í•¨
ls data/test.csv   # ìˆì–´ì•¼ í•¨
```

### 2. H1: XGBoost Baseline ì‹¤í–‰

**ëª©ì **: Lasso ëŒ€ë¹„ XGBoost ì„±ëŠ¥ í™•ì¸

```bash
uv run python experiments/005/run_experiments.py --hypothesis H1
```

**ì˜ˆìƒ ì¶œë ¥**:
```
[INFO] Starting H1: XGBoost Baseline
[INFO] Loading data... (8990 rows)
[INFO] Features: 98 (exclude date_id, targets, lagged)
[INFO] Target: market_forward_excess_returns

[INFO] Cross-validation (TimeSeriesSplit, 5 folds)
Fold 1/5: Sharpe 0.72, Vol Ratio 1.08, k=100
Fold 2/5: Sharpe 0.68, Vol Ratio 1.12, k=100
Fold 3/5: Sharpe 0.75, Vol Ratio 1.05, k=100
Fold 4/5: Sharpe 0.71, Vol Ratio 1.10, k=100
Fold 5/5: Sharpe 0.69, Vol Ratio 1.09, k=100

[RESULT] H1: XGBoost k=100
  Sharpe: 0.71 Â± 0.03
  Vol Ratio: 1.09 Â± 0.03
  vs Lasso (0.604): +17.5%

[INFO] Feature importance saved: results/h1_feature_importance.csv
[INFO] Results saved: results/h1_xgboost_folds.csv
```

**í™•ì¸ í¬ì¸íŠ¸**:
- Sharpe > 0.7? â†’ ì„±ê³µ!
- Feature importance ìƒìœ„ 10ê°œ í™•ì¸
- Lasso Top-20ê³¼ ë¹„êµ

### 3. H2: LightGBM Baseline ì‹¤í–‰

```bash
uv run python experiments/005/run_experiments.py --hypothesis H2
```

**í™•ì¸ í¬ì¸íŠ¸**:
- XGBoostì™€ ì„±ëŠ¥ ë¹„êµ (Â±0.05 ë²”ìœ„)
- í›ˆë ¨ ì†ë„ ë¹„êµ
- Feature importance íŒ¨í„´ ë¹„êµ

### 4. H3: Feature Engineering ì‹¤í–‰

**ëª©ì **: Lag/Rolling features ì¶”ê°€ íš¨ê³¼ í™•ì¸

```bash
uv run python experiments/005/run_experiments.py --hypothesis H3
```

**ìƒì„±ë˜ëŠ” Features**:
- Lag features: M4_lag1, M4_lag5, V13_lag10 ë“±
- Rolling mean: M4_rolling_mean_5, V13_rolling_mean_10 ë“±
- Rolling std: M4_rolling_std_5 ë“±
- Diff: M4_diff1 ë“±

**í™•ì¸ í¬ì¸íŠ¸**:
- Sharpe > H1 + 0.05?
- Feature importanceì— lag/rolling ì§„ì…?
- ì•ˆì •ì„±(Sharpe Std) ê°œì„ ?

### 5. ê²°ê³¼ ë¶„ì„

```bash
# ì „ì²´ ìš”ì•½ í™•ì¸
cat experiments/005/results/summary.csv

# Feature importance í™•ì¸
cat experiments/005/results/h1_feature_importance.csv | head -20

# í´ë“œë³„ ìƒì„¸ ê²°ê³¼
cat experiments/005/results/h1_xgboost_folds.csv
```

---

## ì˜ˆìƒ ê²°ê³¼ í•´ì„

### Case A: Sharpe 0.7~0.85 (ëª©í‘œ ë‹¬ì„±) âœ…
- **í•´ì„**: XGBoostê°€ Lasso ëŒ€ë¹„ 15~40% í–¥ìƒ
- **ë‹¤ìŒ ë‹¨ê³„**:
  1. Best model (H1~H3 ì¤‘)ë¡œ Kaggle ì œì¶œ
  2. k ê°’ ë¯¸ì„¸ ì¡°ì • (best_k Â± 50)
  3. H4~H6 ì„ íƒì  ì‹œë„

### Case B: Sharpe 0.85~1.0 (ëŒ€ì„±ê³µ) ğŸ‰
- **í•´ì„**: Feature engineering íš¨ê³¼ íƒì›”
- **ë‹¤ìŒ ë‹¨ê³„**:
  1. ì¦‰ì‹œ Kaggle ì œì¶œ
  2. Ensemble (H5) ì‹œë„
  3. Regime-based (H6) ê³ ë ¤

### Case C: Sharpe < 0.7 (ì‹¤íŒ¨) âš ï¸
- **í•´ì„**: XGBoostë„ ì˜ˆì¸¡ë ¥ ë¶€ì¡±
- **ë‹¤ìŒ ë‹¨ê³„**:
  1. Feature importance ì¬ë¶„ì„
  2. Hyperparameter tuning
  3. ë‹¤ë¥¸ ì ‘ê·¼ ê³ ë ¤ (Deep Learning, ì™¸ë¶€ ë°ì´í„° ë“±)

---

## ì œì¶œ íŒŒì¼ ìƒì„±

### Best model ì„ íƒ
```bash
# Summaryì—ì„œ ìµœê³  Sharpe ëª¨ë¸ í™•ì¸
cat experiments/005/results/summary.csv | sort -t',' -k2 -rn | head -1

# ì˜ˆ: H3_k100ì´ ìµœê³  (Sharpe 0.82)
```

### Kaggle ì œì¶œìš© íŒŒì¼ ìƒì„±
```bash
# ë°©ë²• 1: ì œì¶œ ìŠ¤í¬ë¦½íŠ¸ ìë™ ìƒì„±
uv run python experiments/005/generate_submission.py --model H3 --k 100

# ë°©ë²• 2: ìˆ˜ë™ìœ¼ë¡œ kaggle_inference ì—…ë°ì´íŠ¸
# kaggle_kernel/kaggle_inference_xgboost.py ìƒì„±
```

### ì œì¶œ
```bash
# Kaggle notebookì— kaggle_inference_xgboost.py ë³µì‚¬
# ì‹¤í–‰ í›„ submission.parquet ìƒì„± í™•ì¸
# Submit to Competition
```

---

## ë””ë²„ê¹… ê°€ì´ë“œ

### ë¬¸ì œ: ModuleNotFoundError
```bash
# í•´ê²°: uvë¡œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install xgboost lightgbm
```

### ë¬¸ì œ: Memory Error
```bash
# í•´ê²°: Feature engineering ë²”ìœ„ ì¶•ì†Œ
# feature_engineering.pyì—ì„œ top_n=10ìœ¼ë¡œ ë³€ê²½ (ê¸°ë³¸ 20)
```

### ë¬¸ì œ: ì‹¤í–‰ ì‹œê°„ ë„ˆë¬´ ê¹€
```bash
# í•´ê²° 1: n_estimators ì¤„ì´ê¸° (500 â†’ 200)
# í•´ê²° 2: í´ë“œ ìˆ˜ ì¤„ì´ê¸° (5 â†’ 3)
# í•´ê²° 3: H1, H3ë§Œ ì‹¤í–‰
uv run python experiments/005/run_experiments.py --hypothesis H1
uv run python experiments/005/run_experiments.py --hypothesis H3
```

### ë¬¸ì œ: XGBoostê°€ Lassoë³´ë‹¤ ë‚®ìŒ
```bash
# ì²´í¬ë¦¬ìŠ¤íŠ¸:
# 1. Feature scaling í•„ìš”? (XGBoostëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ í™•ì¸)
# 2. Hyperparameter ì¡°ì • í•„ìš”?
# 3. Overfitting? (max_depth ì¤„ì´ê¸°, subsample ì¤„ì´ê¸°)
# 4. Data leakage? (lagged features ì œì™¸ í™•ì¸)
```

---

## íŒŒì¼ êµ¬ì¡°

```
experiments/005/
â”œâ”€â”€ README.md                    # ì´ íŒŒì¼
â”œâ”€â”€ HYPOTHESES.md               # ê°€ì„¤ ë° ì‹¤í—˜ ê³„íš
â”œâ”€â”€ REPORT.md                   # ì‹¤í—˜ ê²°ê³¼ (ì‹¤í–‰ í›„ ìƒì„±)
â”œâ”€â”€ STRATEGY_PIVOT.md           # ì „ëµ ì „í™˜ ë°°ê²½
â”œâ”€â”€ run_experiments.py          # ë©”ì¸ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ feature_engineering.py      # Feature ìƒì„± í•¨ìˆ˜
â”œâ”€â”€ models.py                   # ëª¨ë¸ ì •ì˜
â”œâ”€â”€ generate_submission.py      # ì œì¶œ íŒŒì¼ ìƒì„±
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ h1_xgboost_folds.csv
â”‚   â”œâ”€â”€ h2_lightgbm_folds.csv
â”‚   â”œâ”€â”€ h3_feature_eng_folds.csv
â”‚   â”œâ”€â”€ h1_feature_importance.csv
â”‚   â””â”€â”€ summary.csv
â””â”€â”€ submissions/
    â””â”€â”€ best_model_submission.csv
```

---

## ì¼ì • (ì¶”ì²œ)

### Day 1 ì˜¤ì „ (2~3ì‹œê°„)
- [ ] H1: XGBoost Baseline ì‹¤í–‰
- [ ] H2: LightGBM Baseline ì‹¤í–‰
- [ ] ê²°ê³¼ ë¶„ì„, Feature importance í™•ì¸

### Day 1 ì˜¤í›„ (2~3ì‹œê°„)
- [ ] H3: Feature Engineering ì‹¤í–‰
- [ ] ê²°ê³¼ ë¹„êµ (H1 vs H2 vs H3)
- [ ] Best model ì„ ì •

### Day 1 ì €ë… (1~2ì‹œê°„)
- [ ] Kaggle ì œì¶œ íŒŒì¼ ìƒì„±
- [ ] ì œì¶œ ë° ì ìˆ˜ í™•ì¸

### Day 2 (ì„ íƒ, ì‹œê°„ ìˆìœ¼ë©´)
- [ ] H4: Interaction features
- [ ] H5: Ensemble
- [ ] H6: Regime-based model

---

## ì„±ê³µ ê¸°ì¤€

### Minimum (ìµœì†Œ)
- âœ… H1, H2, H3 ì‹¤í–‰ ì™„ë£Œ
- âœ… CV Sharpe > 0.7 (Lasso ëŒ€ë¹„ +15%)
- âœ… Kaggle ì œì¶œ 1íšŒ

### Target (ëª©í‘œ)
- âœ… CV Sharpe > 0.85 (+40%)
- âœ… Kaggle ì ìˆ˜ > 3.0 (í˜„ì¬ 0.44 ëŒ€ë¹„ 7ë°°)

### Stretch (ë„ì „)
- âœ… CV Sharpe > 1.0 (+65%)
- âœ… Kaggle ì ìˆ˜ > 5.0 (11ë°°)
- âœ… ìƒìœ„ 50% ì§„ì…

---

## ì°¸ê³ 

- EXP-000: Feature ë¶„ì„
- EXP-002: Lasso baseline (Sharpe 0.604)
- EXP-004: k ì¡°ì • ì‹¤íŒ¨ (Kaggle 0.15~0.44)
- **EXP-005: ëª¨ë¸ ì „í™˜** (XGBoost/LightGBM)

**í•µì‹¬**: k íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ **ëª¨ë¸ ìì²´ë¥¼ ë°”ê¿”ì•¼ 17+ ë‹¬ì„± ê°€ëŠ¥**

---

## ì§ˆë¬¸/ì´ìŠˆ

ë¬¸ì œ ë°œìƒ ì‹œ:
1. GitHub Issuesì— ë¦¬í¬íŠ¸
2. ë˜ëŠ” ì‹¤í—˜ ë¡œê·¸ ê³µìœ 
